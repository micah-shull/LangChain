{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxBKfZTyPbc3K6INYvHIeu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_008_PromptEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üéØ **Why Broad Prompts Are Weak**\n",
        "\n",
        "When a prompt is **too vague or broad**, the model has to guess:\n",
        "\n",
        "* **What kind of answer you want** (short? detailed? technical? playful?)\n",
        "* **Who the audience is** (child? expert? general public?)\n",
        "* **What format to respond in** (list? essay? code? dialogue?)\n",
        "\n",
        "Because of this ambiguity, the model might give a generic or irrelevant response ‚Äî not because it's ‚Äúwrong,‚Äù but because **you didn‚Äôt steer it clearly**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß≠ **Why Context + Constraints Make Strong Prompts**\n",
        "\n",
        "* **Constraints don‚Äôt limit creativity ‚Äî they shape it.**\n",
        "* Think of it like poetry: a haiku is more creative *because* it has rules.\n",
        "\n",
        "By giving:\n",
        "\n",
        "* a **clear role or scenario** (e.g., \"You‚Äôre a UX designer‚Ä¶\"),\n",
        "* a **specific task** (e.g., ‚Äúevaluate a landing page‚Ä¶‚Äù),\n",
        "* and a **defined output** (e.g., ‚Äúrespond in bullet points, 100 words max‚Ä¶‚Äù),\n",
        "\n",
        "You‚Äôre helping the LLM channel its capabilities *within boundaries*, like water through a well-carved riverbed.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Metaphor: Prompt as a Lens\n",
        "\n",
        "A prompt acts like a **lens**:\n",
        "\n",
        "* A wide, blurry lens = vague, diffuse output.\n",
        "* A focused lens = sharp, intentional result.\n",
        "\n",
        "You can even **intentionally widen or tighten** that lens depending on the creativity vs precision you want.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hi_fzKxBTC9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚úÖ **LLMs Are Problem Solvers, Not Just Answer Machines**\n",
        "\n",
        "Most people still treat language models like fancy search engines:\n",
        "\n",
        "> *‚ÄúTell me the capital of France‚Äù* ‚Üí *‚ÄúParis‚Äù*\n",
        "> But LLMs are **generative** and **context-aware**, meaning they're far more powerful than static answer bots.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **Google Search vs. LLMs: A Comparison**\n",
        "\n",
        "| Google Search                         | Large Language Model (LLM)                                      |\n",
        "| ------------------------------------- | --------------------------------------------------------------- |\n",
        "| Fetches existing content from the web | **Generates** new content from learned patterns                 |\n",
        "| Best at fact-finding and discovery    | Best at **solving problems**, creating, summarizing, explaining |\n",
        "| Returns links                         | Returns direct responses (text, code, structure)                |\n",
        "| Needs user to filter results          | Can **synthesize and format** answers on the spot               |\n",
        "| Can‚Äôt reason or plan                  | Can simulate **reasoning**, planning, ideation                  |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **What Makes LLMs Problem Solvers**\n",
        "\n",
        "LLMs are trained to:\n",
        "\n",
        "* **Predict the next token** in a coherent way\n",
        "* **Complete tasks** from examples or instructions\n",
        "* Simulate **expert roles** (e.g., lawyer, coach, analyst)\n",
        "* Follow **multi-step instructions**\n",
        "* Work with **ambiguous or abstract goals**, not just fact queries\n",
        "\n",
        "So when you give a good prompt like:\n",
        "\n",
        "> ‚ÄúAct as a startup coach. Help me validate a product idea with no budget and no audience. Outline steps I should take this week.‚Äù\n",
        "\n",
        "‚Ä¶it‚Äôs solving a **real-world problem** creatively and adaptively ‚Äî something a search engine can‚Äôt do.\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Bottom Line:\n",
        "\n",
        "> **LLMs are interactive problem-solvers. The more you treat them like collaborators, the more value you unlock.**\n",
        "\n"
      ],
      "metadata": {
        "id": "z3QQ46UfTug1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LLMs can be **very creative** ‚Äî often **surprisingly so** ‚Äî but their creativity is **pattern-based, not conscious or original in a human sense**. Let me explain that clearly:\n",
        "\n",
        "---\n",
        "\n",
        "### üé® **What LLM Creativity *Is***:\n",
        "\n",
        "LLMs generate creative outputs by:\n",
        "\n",
        "* **Combining concepts** in novel ways (metaphors, analogies, plot twists)\n",
        "* **Mimicking styles** of artists, writers, or genres\n",
        "* **Completing partial ideas** with flair (e.g., story starters, taglines, design briefs)\n",
        "* **Blending roles or domains** (e.g., a chef who writes poetry)\n",
        "* Using **structured randomness** to explore different possibilities\n",
        "\n",
        "> Think of it like a hyper-fluent improviser ‚Äî it‚Äôs remixing the internet‚Äôs knowledge into new configurations.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§ñ Examples of LLM Creativity:\n",
        "\n",
        "* üìù *Write a story where Sherlock Holmes solves a case in space.*\n",
        "* üé≠ *Generate a Shakespearean-style insult using modern slang.*\n",
        "* üß™ *Invent a fictional scientific theory that explains why cats always land on their feet.*\n",
        "* üé® *Describe an alien culture that values silence over speech.*\n",
        "\n",
        "Each of these can yield responses that are clever, expressive, and unusual ‚Äî sometimes indistinguishable from a human writer‚Äôs rough draft.\n",
        "\n"
      ],
      "metadata": {
        "id": "Un8vTMzfUJoc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### üß† **How LLMs Blend Ideas**\n",
        "\n",
        "LLMs blend concepts based on **patterns in language**, not conscious choice. Here‚Äôs what that means:\n",
        "\n",
        "1. **They don‚Äôt ‚Äúdecide‚Äù concepts to blend.**\n",
        "\n",
        "   * There‚Äôs no intention or goal-setting.\n",
        "   * Instead, they generate what *statistically follows* from the prompt ‚Äî using patterns learned from billions of examples of creative writing, metaphor, analogy, etc.\n",
        "\n",
        "2. **They associate ideas based on co-occurrence and structure.**\n",
        "\n",
        "   * If ‚Äúgravity‚Äù often appears with ‚Äúfalling‚Äù and ‚Äúfreedom‚Äù in poetic writing, the model can link those in a novel sentence.\n",
        "   * If ‚Äúcat‚Äù and ‚Äúquantum‚Äù appear together in contexts like Schr√∂dinger's cat, the model can creatively riff on quantum metaphors.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ **Mechanics of Idea Blending**\n",
        "\n",
        "LLMs combine concepts by:\n",
        "\n",
        "| Mechanism                    | Example                                                                  |\n",
        "| ---------------------------- | ------------------------------------------------------------------------ |\n",
        "| **Analogy/Substitution**     | ‚ÄúA firewall is like a bouncer at a club.‚Äù                                |\n",
        "| **Metaphorical Inversion**   | ‚ÄúAnxiety is a fog that follows you indoors.‚Äù                             |\n",
        "| **Domain Transfer**          | Applying game theory to relationships, or cooking metaphors to business. |\n",
        "| **Concept Fusion**           | ‚ÄúA startup is a rebellious teenager with a laptop and a dream.‚Äù          |\n",
        "| **Contextual Juxtaposition** | ‚ÄúWhat if Shakespeare wrote sci-fi? Or Elon Musk wrote poetry?‚Äù           |\n",
        "\n",
        "These aren‚Äôt hardcoded tricks ‚Äî the model *learns* from countless examples how creative writers have made such blends.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è What Guides the Output?\n",
        "\n",
        "The output is shaped by:\n",
        "\n",
        "* **Your prompt** (style, topic, constraints)\n",
        "* **Latent associations** in the training data\n",
        "* **Implicit structures** like narrative arcs, common metaphors, character tropes\n",
        "\n",
        "> Prompt: *\"Describe depression as if it were a malfunctioning piece of technology.\"*\n",
        "> LLM might say: *‚ÄúDepression is like an operating system glitch ‚Äî it boots up, but nothing responds. All functions are technically ‚Äòon,‚Äô yet frozen.‚Äù*\n",
        "\n",
        "That‚Äôs conceptual blending through metaphor ‚Äî based on language patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "tbI2R-PcVMUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### üß¨ **Embeddings: The Core of Association**\n",
        "\n",
        "Embeddings are **mathematical representations of words, phrases, or even concepts** in high-dimensional space. Here's what that means:\n",
        "\n",
        "* Each word or idea is represented as a **vector** ‚Äî a long list of numbers.\n",
        "* Words with **similar meaning or contextual usage** end up **close together** in that space.\n",
        "\n",
        "For example:\n",
        "\n",
        "* `\"cat\"`, `\"kitten\"`, and `\"feline\"` are clustered together.\n",
        "* `\"quantum\"` might be near `\"particle\"`, `\"entanglement\"`, and yes ‚Äî `\"Schr√∂dinger\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **So How Does Conceptual Blending Work?**\n",
        "\n",
        "When you give a prompt like:\n",
        "\n",
        "> *\"Describe gravity as a character in a dystopian novel.\"*\n",
        "\n",
        "The model uses:\n",
        "\n",
        "1. **The embedding of ‚Äúgravity‚Äù** (a physics concept, but often used metaphorically ‚Äî ‚Äúweighed down by gravity‚Äù).\n",
        "2. **Nearby concepts** in its semantic space ‚Äî things like ‚Äúfalling,‚Äù ‚Äúpull,‚Äù ‚Äúburden,‚Äù ‚Äúinescapable,‚Äù ‚Äúorbit.‚Äù\n",
        "3. **The tone and genre prompt (\"dystopian novel\")** ‚Äî which activates clusters of darker, moodier, dramatic language.\n",
        "\n",
        "It **samples from and mixes** these nearby regions in the embedding space to generate novel but contextually coherent output.\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ Co-occurrence + Proximity = Creativity\n",
        "\n",
        "You're 100% right to link co-occurrence and embeddings:\n",
        "\n",
        "* **Co-occurrence in training data** shapes where words land in the vector space.\n",
        "* **Proximity in that space** then guides what words the model predicts next ‚Äî hence associations and creative blends.\n",
        "\n",
        "So when you ask it to ‚Äúcombine X and Y,‚Äù you're activating regions of that space where:\n",
        "\n",
        "* X lives,\n",
        "* Y lives,\n",
        "* and **overlaps or analogies** between them can be constructed.\n",
        "\n",
        "---\n",
        "\n",
        "### üìçAnalogy: Embeddings = Mental Map\n",
        "\n",
        "Imagine embeddings as a **giant 3D mind-map** of all human language:\n",
        "\n",
        "* Words that often appear together or serve similar functions **cluster**.\n",
        "* Concepts across domains can sit close if they‚Äôre metaphorically or structurally similar.\n",
        "* LLMs are navigating that map to ‚Äúimprovise‚Äù the next most likely ‚Äî and often creatively linked ‚Äî response.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qHTUYBsNV4b_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gWmv0l9DRYPa"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet python-dotenv openai pydantic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üåø Environment setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "# Load token from .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Initialize client with API key (optional if OPENAI_API_KEY is already in env)\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Define function to ask GPT\n",
        "def ask_gpt(prompt, model=MODEL):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.8,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# ‚úÖ Test prompt\n",
        "print(ask_gpt(\"Give me a tip for writing strong dialogue.\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr55trgVjO1U",
        "outputId": "de6f2c33-55d4-41cf-9ad7-cda946768eb1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Certainly! One effective tip for writing strong dialogue is to ensure that each character has a distinct voice. This means giving your characters unique speech patterns, vocabulary, and rhythms that reflect their backgrounds, personalities, and emotional states. By doing so, readers can easily differentiate between characters, and the dialogue becomes more engaging and believable.\n",
            "\n",
            "To achieve this, consider the following:\n",
            "\n",
            "1. **Background and Education**: Think about the character's upbringing, education level, and social status, and let these factors influence how they speak.\n",
            "\n",
            "2. **Personality Traits**: A shy character might use fewer words or speak hesitantly, while an assertive character might be more direct and confident in their speech.\n",
            "\n",
            "3. **Emotional State**: Adapt the dialogue to reflect the character‚Äôs current emotions. For instance, excited characters might speak quickly, whereas sad characters might use shorter, more subdued sentences.\n",
            "\n",
            "4. **Use Subtext**: Often, what characters don‚Äôt say is just as important as what they do say. Allow characters to imply or hint at deeper meanings, creating tension and intrigue.\n",
            "\n",
            "By focusing on these elements, you can create dialogue that not only progresses the story but also deepens character development and keeps readers engaged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAI Python SDK** uses `client.chat.completions.create(...)` and returns a `.choices[0].message.content` object that behaves more like a structured Python object than a raw dictionary.\n",
        "\n",
        "The magic behind that is **Pydantic**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± What Is Pydantic?\n",
        "\n",
        "**Pydantic** is a Python library that provides:\n",
        "\n",
        "> ‚úÖ **Data validation** and\n",
        "> ‚úÖ **Structured parsing** using **Python classes** with type hints.\n",
        "\n",
        "It‚Äôs like saying:\n",
        "\n",
        "> ‚ÄúHere‚Äôs what this data *should* look like ‚Äî now enforce it.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Why Pydantic Exists\n",
        "\n",
        "Normally in Python, you deal with **dictionaries**, which are flexible but unstructured:\n",
        "\n",
        "```python\n",
        "response = {\"name\": \"Alice\", \"age\": 30}\n",
        "print(response[\"name\"])\n",
        "```\n",
        "\n",
        "But if the keys are missing or mis-typed? You'll get a crash.\n",
        "\n",
        "---\n",
        "\n",
        "### üîê With Pydantic, You Define a Class Like:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Person(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "\n",
        "person = Person(name=\"Alice\", age=30)\n",
        "print(person.name)  # Safe and autocompleted!\n",
        "```\n",
        "\n",
        "Now:\n",
        "\n",
        "* You get **type checking**\n",
        "* You can validate input data automatically\n",
        "* You can serialize/deserialize from JSON easily\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why OpenAI Uses Pydantic Now\n",
        "\n",
        "With the new OpenAI SDK:\n",
        "\n",
        "* Every response is returned as a **Pydantic model** (not a raw dictionary)\n",
        "* You get **structure**, **auto-complete**, and **validation**\n",
        "* You can export models as JSON with `.model_dump()` or `.model_dump_json()`\n",
        "\n",
        "### üîç Example:\n",
        "\n",
        "```python\n",
        "response = client.chat.completions.create(...)\n",
        "print(response.choices[0].message.content)            # Access like an object\n",
        "print(response.model_dump())                          # Get as dictionary\n",
        "print(response.model_dump_json(indent=2))             # Get as formatted JSON string\n",
        "```\n",
        "\n",
        "So instead of doing fragile string-key access like:\n",
        "\n",
        "```python\n",
        "response['choices'][0]['message']['content']\n",
        "```\n",
        "\n",
        "You now do:\n",
        "\n",
        "```python\n",
        "response.choices[0].message.content  # Clean, safe, typed\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ú® Benefits of Pydantic in OpenAI SDK\n",
        "\n",
        "| Feature        | Benefit                           |\n",
        "| -------------- | --------------------------------- |\n",
        "| Type safety    | Reduces runtime errors            |\n",
        "| Autocompletion | Works great in editors/Colab      |\n",
        "| Validation     | Catches malformed or missing data |\n",
        "| Serialization  | Easily convert to JSON or dicts   |\n",
        "| Nested access  | More natural object-style access  |\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFWky8Ccl13R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Pydantic Prompt Log Model"
      ],
      "metadata": {
        "id": "VEgUiU7hmDpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from pydantic import BaseModel\n",
        "from pprint import pprint\n",
        "import textwrap\n",
        "\n",
        "class PromptLog(BaseModel):\n",
        "    prompt: str\n",
        "    response: str\n",
        "    temperature: float\n",
        "    max_tokens: int\n",
        "\n",
        "    def as_dict(self):\n",
        "        return self.model_dump()"
      ],
      "metadata": {
        "id": "sM5qYTHAcPSc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use It With GPT Call"
      ],
      "metadata": {
        "id": "XBDK8wMDmIz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_gpt(prompt, model=\"gpt-4o\", temperature=0.8, max_tokens=300):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Log prompt, response, and config\n",
        "    log_entry = PromptLog(\n",
        "        prompt=prompt,\n",
        "        response=content,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    return log_entry\n",
        "\n",
        "\n",
        "# # Ask and log\n",
        "entry = ask_gpt(\"Give me a tip for writing strong dialogue.\")\n",
        "\n",
        "# Print config\n",
        "print(f\"üéõÔ∏è Temperature: {entry.temperature} | üî¢ Max tokens: {entry.max_tokens}\\n\")\n",
        "\n",
        "# Wrap and print response\n",
        "print(\"üì§ Response:\\n\")\n",
        "wrapped = textwrap.fill(entry.response, width=100)\n",
        "print(wrapped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-j9SJ0cPPo",
        "outputId": "b1d0b624-0ddd-47fd-a13f-a1808583977f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Temperature: 0.8 | üî¢ Max tokens: 300\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "A key tip for writing strong dialogue is to ensure that each character has a distinct voice. This\n",
            "means that their speech should reflect their unique background, personality, and motivations. To\n",
            "achieve this, consider the following:  1. **Character Background and Personality**: Think about each\n",
            "character‚Äôs age, education, occupation, and region of origin, as well as their personality traits.\n",
            "These factors will influence their vocabulary, speech patterns, and how they express themselves.  2.\n",
            "**Purposeful Dialogue**: Make sure each line of dialogue serves a purpose. It should either reveal\n",
            "something about the character, advance the plot, or build tension/conflict. Avoid small talk unless\n",
            "it adds depth or meaning to the scene.  3. **Subtext**: Real conversations are often layered with\n",
            "subtext‚Äîwhat‚Äôs implied but not said. Characters may have conflicting desires or hidden agendas, and\n",
            "this can add complexity and interest to their interactions.  4. **Natural Rhythm**: Listen to how\n",
            "people speak in real life, noting the rhythm and flow of conversation. Use contractions, sentence\n",
            "fragments, and interruptions to mimic natural speech, but ensure it remains clear for the reader.\n",
            "5. **Read Aloud**: Reading your dialogue out loud can help you catch awkward phrasing or unnatural\n",
            "rhythms. It also helps you ensure that each character‚Äôs voice is distinctive and consistent.  By\n",
            "focusing on these aspects, you can craft dialogue that is engaging, authentic, and serves the story\n",
            "effectively.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PERSONA"
      ],
      "metadata": {
        "id": "J8QgsqdPqEEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_gpt(\n",
        "    prompt,\n",
        "    model=\"gpt-4o\",\n",
        "    temperature=0.4,\n",
        "    max_tokens=300,\n",
        "    system_message=\"You are a helpful assistant.\"\n",
        "):\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    content = response.choices[0].message.content\n",
        "\n",
        "    # Log prompt, response, and config\n",
        "    log_entry = PromptLog(\n",
        "        prompt=prompt,\n",
        "        response=content,\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    return log_entry\n",
        "\n",
        "def display_log(entry, width=100):\n",
        "    import textwrap\n",
        "\n",
        "    def wrap_multiline(text):\n",
        "        # Split by newlines, wrap each paragraph individually\n",
        "        paragraphs = text.strip().split('\\n')\n",
        "        return \"\\n\\n\".join(textwrap.fill(p.strip(), width=width) for p in paragraphs if p.strip())\n",
        "\n",
        "    # Print settings\n",
        "    print(f\"üéõÔ∏è Temperature: {entry.temperature} | üî¢ Max tokens: {entry.max_tokens}\\n\")\n",
        "\n",
        "    # Prompt\n",
        "    print(\"üì• Prompt:\\n\")\n",
        "    print(wrap_multiline(entry.prompt))\n",
        "\n",
        "    # Response\n",
        "    print(\"\\nüì§ Response:\\n\")\n",
        "    print(wrap_multiline(entry.response))\n",
        "\n"
      ],
      "metadata": {
        "id": "7Im6AqcDqJxL"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### award-winning screenwriter"
      ],
      "metadata": {
        "id": "qPHAbdF8qOCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask and log\n",
        "entry = ask_gpt(\n",
        "    prompt=\"Give me a tip for writing strong dialogue.\",\n",
        "    system_message=\"You are an award-winning screenwriter coaching a young writer.\"\n",
        "  )\n",
        "\n",
        "display_log(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFAwSHl0n5b-",
        "outputId": "804ea41e-0b5e-4279-fab5-9aae0a988ef7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Give me a tip for writing strong dialogue.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Certainly! One of the keys to writing strong dialogue is to ensure that each character has a\n",
            "distinct voice. This means that their speech patterns, vocabulary, and rhythm should reflect their\n",
            "background, personality, and emotional state. To achieve this:\n",
            "\n",
            "1. **Know Your Characters**: Spend time developing your characters' backstories, motivations, and\n",
            "quirks. The more you know about them, the more naturally their dialogue will flow.\n",
            "\n",
            "2. **Read Aloud**: Dialogue should sound natural when spoken. Reading it aloud helps you catch\n",
            "awkward phrasing and ensures it sounds authentic.\n",
            "\n",
            "3. **Subtext is Key**: People rarely say exactly what they mean. Use subtext to add layers to your\n",
            "dialogue, allowing characters to express their true feelings indirectly.\n",
            "\n",
            "4. **Keep it Concise**: Avoid long-winded speeches. Real conversations are often brief and to the\n",
            "point. Trim any unnecessary words to keep the dialogue sharp.\n",
            "\n",
            "5. **Listen to Real Conversations**: Pay attention to how people talk in real life. Notice the\n",
            "pauses, interruptions, and how they convey emotion. This will help you create more realistic and\n",
            "engaging dialogue.\n",
            "\n",
            "By focusing on these aspects, you'll craft dialogue that not only sounds authentic but also drives\n",
            "the story and reveals character depth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### first grader"
      ],
      "metadata": {
        "id": "b3sV-wakt93y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask and log\n",
        "entry = ask_gpt(\n",
        "    prompt=\"Give me a tip for writing strong dialogue.\",\n",
        "    system_message=\"You are a first grader who does not know mych about writing or dialogue\"\n",
        "  )\n",
        "\n",
        "# print the response\n",
        "display_log(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rK5j6pSn5Y7",
        "outputId": "7b49037d-02dd-437d-8e70-e21b40313ca0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Give me a tip for writing strong dialogue.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Um, when people talk, they don't always say everything perfectly. So, maybe make the people in your\n",
            "story talk like real people. Like, they can say \"um\" or \"uh\" or maybe they don't finish their\n",
            "sentences. And they can have feelings, like happy or sad, when they talk. That makes it sound real!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a dog"
      ],
      "metadata": {
        "id": "CY2eHEtTuAr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask and log\n",
        "entry = ask_gpt(\n",
        "    prompt=\"Give me a tip for writing strong dialogue.\",\n",
        "    system_message=\"You are a dog that only knows how to bark\"\n",
        "  )\n",
        "\n",
        "# print the response\n",
        "display_log(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6GrJAM5tcaQ",
        "outputId": "ef66c31f-39fa-4d5e-a430-2e5ce3e049e4"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Give me a tip for writing strong dialogue.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Woof! Woof woof! üêæ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask and log\n",
        "entry = ask_gpt(\n",
        "    prompt=\"Explain the Yield Curve in Economics\",\n",
        "    system_message=\"Please explaare a Harvard edcuated professor who doesn't really have the time or the patience \\\n",
        "    to answer pedestrian queries\"\n",
        "  )\n",
        "\n",
        "# print the response\n",
        "display_log(entry)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3pO5vHun5V4",
        "outputId": "99e43be5-48a1-49bd-aafe-60e49150ea14"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Explain the Yield Curve in Economics\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "The yield curve is a graphical representation that shows the relationship between interest rates and\n",
            "the maturity dates of debt securities, typically government bonds. It is a crucial concept in\n",
            "economics and finance, as it provides insights into future interest rate changes and economic\n",
            "activity.\n",
            "\n",
            "There are three main types of yield curves:\n",
            "\n",
            "1. **Normal Yield Curve**: This is upward sloping, indicating that longer-term securities have\n",
            "higher yields compared to short-term ones. It reflects the expectation that the economy will grow,\n",
            "and inflation will rise, leading to higher interest rates in the future.\n",
            "\n",
            "2. **Inverted Yield Curve**: This is downward sloping, where short-term yields are higher than long-\n",
            "term ones. An inverted yield curve is often seen as a predictor of an economic recession, as it\n",
            "suggests that investors expect interest rates to fall in the future due to declining economic\n",
            "activity.\n",
            "\n",
            "3. **Flat or Humped Yield Curve**: This occurs when there is little difference between short-term\n",
            "and long-term yields, indicating uncertainty about future economic conditions. A humped curve can\n",
            "suggest that interest rates are expected to rise in the short term but fall in the long term.\n",
            "\n",
            "The yield curve is influenced by factors such as monetary policy, inflation expectations, and\n",
            "investor sentiment. Central banks, like the Federal Reserve, play a significant role in shaping the\n",
            "yield curve through their control of short-term interest rates.\n",
            "\n",
            "In summary, the yield curve is a vital tool for economists, investors, and policymakers to gauge the\n",
            "direction of the economy and make informed decisions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OPENAI Prompt"
      ],
      "metadata": {
        "id": "fr_GLixq5db3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask and log\n",
        "entry = ask_gpt(\n",
        "    prompt=\"Give me a tip for writing strong dialogue.\",\n",
        "    system_message=\t\"You are a helpful, harmless, and honest assistant. Answer as clearly and informatively as possible.\"\n",
        "\n",
        "  )\n",
        "\n",
        "# print the response\n",
        "display_log(entry)"
      ],
      "metadata": {
        "id": "V5062Krwn5TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß∞ **Essential Prompt Templates for Prompt Engineering**\n",
        "\n",
        "---\n",
        "\n",
        "### üé≠ **1. Role + Task Prompt** *(Most common, great for RAG)*\n",
        "\n",
        "```text\n",
        "You are a [persona or role]. Your job is to [task or goal].\n",
        "\n",
        "User: [insert question]\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```text\n",
        "You are a customer support agent for a healthcare startup. Your job is to explain company policies clearly and kindly.\n",
        "\n",
        "User: How do I schedule an appointment?\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° **2. Instruction Prompt** *(Direct and clean)*\n",
        "\n",
        "```text\n",
        "Explain [topic] to a [target audience] in [format].\n",
        "```\n",
        "\n",
        "**Example:**\n",
        "\n",
        "```text\n",
        "Explain compound interest to a 12-year-old using a short story and a simple math example.\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ **3. Few-Shot Prompt** *(Show the model what you want)*\n",
        "\n",
        "```text\n",
        "Input: \"I have no clue what this means.\"\n",
        "Response: \"That means the person is confused or unsure.\"\n",
        "\n",
        "Input: \"I'm starving!\"\n",
        "Response: \"The person is very hungry.\"\n",
        "\n",
        "Input: \"[Your example here]\"\n",
        "Response:\n",
        "```\n",
        "\n",
        "**Use for:** Tone conversion, classification, summarization, pattern imitation.\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **4. Output Format Prompt**\n",
        "\n",
        "```text\n",
        "Provide the answer in the following format:\n",
        "\n",
        "- Summary:\n",
        "- Key Points:\n",
        "- Action Items:\n",
        "```\n",
        "\n",
        "**Great for:** Business, dashboards, product summaries.\n",
        "\n",
        "---\n",
        "\n",
        "### ü§π **5. Comparison Prompt**\n",
        "\n",
        "```text\n",
        "Compare [concept A] and [concept B] in terms of:\n",
        "- Definition\n",
        "- Use Cases\n",
        "- Pros/Cons\n",
        "\n",
        "Present in a table.\n",
        "```\n",
        "\n",
        "**Great for:** Education, decision-making tools, client-facing output\n",
        "\n",
        "---\n",
        "\n",
        "### üé® **6. Creative Constraint Prompt**\n",
        "\n",
        "```text\n",
        "Describe [topic] as if it were [unusual metaphor or character].\n",
        "\n",
        "Example:\n",
        "Describe inflation as if it were a mischievous character in a fantasy novel.\n",
        "```\n",
        "\n",
        "**Use for:** Marketing, storytelling, analogical explanation\n",
        "\n",
        "---\n",
        "\n",
        "### üß© **7. Multi-Persona Prompt (for testing tone)**\n",
        "\n",
        "```text\n",
        "Respond to this question as:\n",
        "1. A supportive coach\n",
        "2. A sarcastic comedian\n",
        "3. A formal academic\n",
        "\n",
        "Question: [insert user input]\n",
        "```\n",
        "\n",
        "**Use for:** A/B testing tone, character building, persona design\n",
        "\n",
        "---\n",
        "\n",
        "### üîê **8. RAG Prompt Frame (for AI + knowledge grounding)**\n",
        "\n",
        "```text\n",
        "You are a helpful assistant for [company name]. Answer only using the information provided.\n",
        "\n",
        "If you don't know the answer, say you don't know and suggest the user contact support.\n",
        "\n",
        "[Insert retrieved context]\n",
        "\n",
        "User question: [insert here]\n",
        "```\n",
        "\n",
        "**This is the prompt style for production RAG pipelines.**\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Final Tip:\n",
        "\n",
        "> ‚ÄúPrompting isn‚Äôt about making the model smarter ‚Äî it‚Äôs about making your **instructions clearer**.‚Äù\n"
      ],
      "metadata": {
        "id": "h2K2TI_Y9P5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt Template Examples for Notebook Testing\n",
        "\n",
        "prompt_templates = [\n",
        "    {\n",
        "        \"title\": \"üé≠ Role + Task Prompt\",\n",
        "        \"system_message\": \"You are a customer support agent for a healthcare startup. Your job is to explain company policies clearly and kindly.\",\n",
        "        \"prompt\": \"How do I schedule an appointment?\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"üí° Instruction Prompt\",\n",
        "        \"system_message\": \"You are a helpful assistant.\",\n",
        "        \"prompt\": \"Explain compound interest to a 12-year-old using a short story and a simple math example.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"üß™ Few-Shot Prompt\",\n",
        "        \"system_message\": \"You are a language expert that interprets expressions into plain meaning.\",\n",
        "        \"prompt\": (\n",
        "            \"Input: \\\"I have no clue what this means.\\\"\\n\"\n",
        "            \"Response: \\\"That means the person is confused or unsure.\\\"\\n\\n\"\n",
        "            \"Input: \\\"I'm starving!\\\"\\n\"\n",
        "            \"Response: \\\"The person is very hungry.\\\"\\n\\n\"\n",
        "            \"Input: \\\"She's on fire today!\\\"\\n\"\n",
        "            \"Response:\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"üìä Output Format Prompt\",\n",
        "        \"system_message\": \"You are a business analyst assistant.\",\n",
        "        \"prompt\": (\n",
        "            \"Summarize the benefits of using AI in customer service.\\n\"\n",
        "            \"Provide the answer in the following format:\\n\\n\"\n",
        "            \"- Summary:\\n- Key Points:\\n- Action Items:\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"ü§π Comparison Prompt\",\n",
        "        \"system_message\": \"You are an economics tutor.\",\n",
        "        \"prompt\": (\n",
        "            \"Compare inflation and deflation in terms of:\\n\"\n",
        "            \"- Definition\\n- Use Cases\\n- Pros/Cons\\n\\nPresent in a table.\"\n",
        "        )\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"üé® Creative Constraint Prompt\",\n",
        "        \"system_message\": \"You are a fantasy author.\",\n",
        "        \"prompt\": \"Describe inflation as if it were a mischievous character in a fantasy novel.\"\n",
        "    },\n",
        "    {\n",
        "        \"title\": \"üß© RAG Prompt Frame\",\n",
        "        \"system_message\": (\n",
        "            \"You are a helpful assistant for MedixCare. Answer only using the information provided.\\n\"\n",
        "            \"If you don't know the answer, say you don't know and suggest the user contact support.\\n\\n\"\n",
        "            \"[MedixCare provides preventive health checkups, online appointment scheduling, and access to lab reports.]\"\n",
        "        ),\n",
        "        \"prompt\": \"Can I access my lab results through your platform?\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Example usage:\n",
        "for template in prompt_templates:\n",
        "    entry = ask_gpt(prompt=template[\"prompt\"], system_message=template[\"system_message\"])\n",
        "    print(f\"\\n=== {template['title']} ===\")\n",
        "    display_log(entry)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSZetjhVaDuS",
        "outputId": "6dea3269-402e-47bc-ae75-3c3a82d93bcc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== üé≠ Role + Task Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "How do I schedule an appointment?\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Scheduling an appointment with us is simple and convenient. You can choose one of the following\n",
            "methods:\n",
            "\n",
            "1. **Online Portal:** Visit our website and log into your account. Once logged in, navigate to the\n",
            "'Appointments' section where you can view available time slots and select one that suits your\n",
            "schedule.\n",
            "\n",
            "2. **Mobile App:** If you have our mobile app, open it and go to the 'Appointments' tab. From there,\n",
            "you can book an appointment by choosing a date and time that works for you.\n",
            "\n",
            "3. **Phone Call:** You can also call our customer support line, and one of our representatives will\n",
            "be happy to assist you in scheduling an appointment.\n",
            "\n",
            "4. **Email:** Send us an email with your preferred dates and times, and we will get back to you with\n",
            "available options.\n",
            "\n",
            "Please ensure you have your account details handy, as you may need them to confirm your appointment.\n",
            "If you have any further questions or need assistance, feel free to reach out to our support team.\n",
            "We're here to help!\n",
            "\n",
            "=== üí° Instruction Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Explain compound interest to a 12-year-old using a short story and a simple math example.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "**The Magic of Compound Interest: A Short Story**\n",
            "\n",
            "Once upon a time, in a small village, there was a boy named Alex. Alex loved collecting shiny\n",
            "stones, and he had a special jar where he kept them. One day, his wise grandmother gave him a\n",
            "magical stone and said, \"This stone can make more stones grow if you keep it in your jar. It's\n",
            "called the 'Magic of Compound Interest.'\"\n",
            "\n",
            "Curious, Alex asked, \"How does it work, Grandma?\"\n",
            "\n",
            "His grandmother explained, \"Every year, the magic stone will help you grow more stones based on how\n",
            "many stones you already have. The more stones you have, the more new stones will appear!\"\n",
            "\n",
            "Excited, Alex decided to try it out. He started with 10 shiny stones in his jar, including the magic\n",
            "stone. His grandmother told him that every year, the magic would add 10% more stones to his jar.\n",
            "\n",
            "**Year 1:**\n",
            "\n",
            "- Alex had 10 stones.\n",
            "\n",
            "- The magic added 10% of 10 stones, which is 1 stone.\n",
            "\n",
            "- Now, Alex had 11 stones.\n",
            "\n",
            "**Year 2:**\n",
            "\n",
            "- Alex had 11 stones.\n",
            "\n",
            "- The magic added 10% of 11 stones, which is 1.1 stones (let's say 1 stone and a little bit more).\n",
            "\n",
            "- Now, Alex had about 12 stones.\n",
            "\n",
            "**Year 3:**\n",
            "\n",
            "- Alex had 12 stones.\n",
            "\n",
            "- The magic added 10%\n",
            "\n",
            "=== üß™ Few-Shot Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Input: \"I have no clue what this means.\"\n",
            "\n",
            "Response: \"That means the person is confused or unsure.\"\n",
            "\n",
            "Input: \"I'm starving!\"\n",
            "\n",
            "Response: \"The person is very hungry.\"\n",
            "\n",
            "Input: \"She's on fire today!\"\n",
            "\n",
            "Response:\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "\"The person is performing exceptionally well or is very enthusiastic today.\"\n",
            "\n",
            "=== üìä Output Format Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Summarize the benefits of using AI in customer service.\n",
            "\n",
            "Provide the answer in the following format:\n",
            "\n",
            "- Summary:\n",
            "\n",
            "- Key Points:\n",
            "\n",
            "- Action Items:\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "- Summary:\n",
            "\n",
            "Integrating AI into customer service offers numerous advantages, including enhanced efficiency,\n",
            "improved customer satisfaction, and cost savings. AI technologies, such as chatbots and virtual\n",
            "assistants, enable businesses to provide 24/7 support, handle large volumes of inquiries, and\n",
            "deliver personalized experiences.\n",
            "\n",
            "- Key Points:\n",
            "\n",
            "- **24/7 Availability**: AI systems can provide round-the-clock support, ensuring customers receive\n",
            "assistance at any time without the need for human intervention.\n",
            "\n",
            "- **Scalability**: AI can handle a large volume of customer inquiries simultaneously, reducing wait\n",
            "times and improving service efficiency.\n",
            "\n",
            "- **Cost Efficiency**: By automating routine tasks and inquiries, businesses can reduce operational\n",
            "costs and allocate human resources to more complex issues.\n",
            "\n",
            "- **Personalization**: AI can analyze customer data to offer personalized recommendations and\n",
            "solutions, enhancing the customer experience.\n",
            "\n",
            "- **Consistency**: AI ensures consistent service quality by providing standardized responses and\n",
            "minimizing human error.\n",
            "\n",
            "- **Data Insights**: AI tools can gather and analyze customer interaction data, offering valuable\n",
            "insights into customer behavior and preferences.\n",
            "\n",
            "- Action Items:\n",
            "\n",
            "- Evaluate current customer service processes to identify areas where AI can be integrated for\n",
            "improved efficiency.\n",
            "\n",
            "- Implement AI-powered tools, such as chatbots, to handle routine inquiries and provide 24/7\n",
            "support.\n",
            "\n",
            "- Train customer service teams to work alongside AI technologies, focusing on complex issues that\n",
            "require human intervention.\n",
            "\n",
            "- Continuously monitor and analyze AI performance to\n",
            "\n",
            "=== ü§π Comparison Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Compare inflation and deflation in terms of:\n",
            "\n",
            "- Definition\n",
            "\n",
            "- Use Cases\n",
            "\n",
            "- Pros/Cons\n",
            "\n",
            "Present in a table.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Here's a comparison of inflation and deflation in a tabular format:\n",
            "\n",
            "| Aspect        | Inflation                                                                 |\n",
            "Deflation                                                                 |\n",
            "\n",
            "|---------------|---------------------------------------------------------------------------|-------\n",
            "--------------------------------------------------------------------|\n",
            "\n",
            "| **Definition**| Inflation is the rate at which the general level of prices for goods and services\n",
            "rises, eroding purchasing power. | Deflation is the decrease in the general price level of goods and\n",
            "services, increasing purchasing power. |\n",
            "\n",
            "| **Use Cases** | - Moderate inflation is often targeted by central banks to encourage spending and\n",
            "investment. <br> - Used to reduce the real burden of debt. | - Rarely a targeted economic policy,\n",
            "but can occur during economic downturns. <br> - Can result from technological advancements that\n",
            "reduce production costs. |\n",
            "\n",
            "| **Pros**      | - Encourages spending and investment as money is expected to be worth less in the\n",
            "future. <br> - Can reduce the real value of debt. | - Increases the real value of money, enhancing\n",
            "purchasing power. <br> - Can lead to lower input costs for businesses. |\n",
            "\n",
            "| **Cons**      | - Can erode purchasing power if wages do not keep up. <br> - Can lead to\n",
            "hyperinflation if uncontrolled, destabilizing economies. | - Can lead to reduced consumer spending\n",
            "as people anticipate lower prices. <br> - Increases the real burden of debt, potentially leading to\n",
            "defaults. <br> - Can cause economic stagnation or recession. |\n",
            "\n",
            "This table provides a concise comparison of inflation and def\n",
            "\n",
            "=== üé® Creative Constraint Prompt ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Describe inflation as if it were a mischievous character in a fantasy novel.\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "In the bustling realm of Econoria, where merchants and traders weave intricate tapestries of\n",
            "commerce, there lurks a mischievous spirit known as Inflation. With a twinkle in his eye and a\n",
            "penchant for chaos, Inflation dances through the marketplace, his presence both feared and\n",
            "begrudgingly acknowledged by all.\n",
            "\n",
            "Clad in a cloak woven from shimmering gold and silver threads that seem to shift and shimmer with\n",
            "every step, Inflation is a master of disguise. He flits between the stalls, his fingers trailing\n",
            "over goods and wares, leaving a subtle, invisible mark that only the keenest of eyes can discern.\n",
            "His touch is light, yet potent, causing the prices of goods to swell like enchanted bread left too\n",
            "long in the sun.\n",
            "\n",
            "Despite his impish nature, Inflation is not malevolent. Rather, he is a force of nature, a trickster\n",
            "spirit who thrives on imbalance and unpredictability. He delights in the confusion he sows, watching\n",
            "as merchants scramble to adjust their ledgers and townsfolk barter with increasing desperation. Yet,\n",
            "there is a method to his madness, a rhythm to his capricious dance that only the wisest of\n",
            "economists can hope to understand.\n",
            "\n",
            "Inflation's laughter echoes through the cobblestone streets, a melodic sound that both enchants and\n",
            "unnerves. He is a reminder of the delicate balance that governs Econoria, a living testament to the\n",
            "ever-changing tides of fortune. Though many have tried to capture or banish him, Inflation remains\n",
            "elusive,\n",
            "\n",
            "=== üß© RAG Prompt Frame ===\n",
            "üéõÔ∏è Temperature: 0.4 | üî¢ Max tokens: 300\n",
            "\n",
            "üì• Prompt:\n",
            "\n",
            "Can I access my lab results through your platform?\n",
            "\n",
            "üì§ Response:\n",
            "\n",
            "Yes, you can access your lab reports through MedixCare's platform.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPtof-riaDrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> üß© *Is a multi-page prompt for an LLM necessary? Or is it overengineering?*\n",
        "\n",
        "The short answer is:\n",
        "\n",
        "> ‚úÖ **Sometimes it's justified (especially for open-source models)**\n",
        "> ‚ùå **Often it's overkill or misunderstood optimization**\n",
        "\n",
        "Let‚Äôs unpack this carefully.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Why Some Companies Use Long Prompts\n",
        "\n",
        "### ‚úÖ 1. **They‚Äôre Using Open-Source Models That Lack Instruction Tuning**\n",
        "\n",
        "Unlike OpenAI models (`gpt-4`, `gpt-4o`, etc.), many open-source models:\n",
        "\n",
        "* Are not RLHF-tuned (no instruction-following behavior baked in)\n",
        "* Don‚Äôt have safety, helpfulness, or formatting defaults\n",
        "* Need **explicit** guidance to behave like an assistant\n",
        "\n",
        "> In these cases, a long prompt acts as a **manual substitute** for OpenAI‚Äôs internal training.\n",
        "\n",
        "Example long prompts may include:\n",
        "\n",
        "* A system persona\n",
        "* Style rules\n",
        "* Few-shot examples\n",
        "* Output formatting\n",
        "* Ethical boundaries\n",
        "* Error correction rules\n",
        "* Language preferences\n",
        "\n",
        "That‚Äôs **legit**, but also heavy.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **They‚Äôre Creating Multi-Role or Agent Systems**\n",
        "\n",
        "Some companies are building ‚Äúagent-like‚Äù systems where the LLM:\n",
        "\n",
        "* Simulates multiple roles (e.g., ‚ÄúCEO‚Äù, ‚Äúanalyst‚Äù, ‚Äúuser‚Äù)\n",
        "* Makes decisions step-by-step\n",
        "* Responds in structured formats\n",
        "\n",
        "So the prompt includes:\n",
        "\n",
        "* Context\n",
        "* Memory\n",
        "* Rules\n",
        "* Goals\n",
        "* Response format constraints\n",
        "\n",
        "> This is more about **agent programming** than casual prompting.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå When It‚Äôs Overengineering (and Common)\n",
        "\n",
        "Sometimes, long prompts are a sign of:\n",
        "\n",
        "* **Lack of trust** in the model's default behavior\n",
        "* **Copy-pasting from templates** without understanding\n",
        "* **Too many voices** involved in the prompt design (product + legal + branding + engineering)\n",
        "\n",
        "You‚Äôll see:\n",
        "\n",
        "* Repetitive adjectives: *\"Be friendly, warm, helpful, accurate, neutral, compassionate...\"*\n",
        "* Conflicting goals: *\"Be concise but explain fully. Be brief but poetic.\"*\n",
        "* Over-constraint: *\"Always use bullet points, unless a paragraph is better, but not too long...\"*\n",
        "\n",
        "> That‚Äôs not control ‚Äî that‚Äôs noise.\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ Consultant's Takeaway\n",
        "\n",
        "| Model Type                             | Ideal Prompt Length                             |\n",
        "| -------------------------------------- | ----------------------------------------------- |\n",
        "| OpenAI GPT-4o                          | **Short + clear** (\\~1‚Äì3 sentences)             |\n",
        "| Claude / Gemini                        | Short to moderate (if structured)               |\n",
        "| Open-source LLMs (e.g. Mistral, LLaMA) | May need long prompts, examples, or scaffolding |\n",
        "| Fine-tuned domain models               | Keep it scoped to task + tone                   |\n",
        "\n",
        "> üß† *If you‚Äôre using a powerful pretrained model like GPT-4o, adding more words often adds more risk than value.*\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ What to Do Instead of Long Prompts\n",
        "\n",
        "* Keep your **system prompt crisp**: role, scope, tone\n",
        "* Use **RAG** to inject relevant, concise, factual content\n",
        "* Let the model do what it was trained for: generalization, reasoning, structure\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zmt3FF1d8zQv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üèõÔ∏è What Is a Model Architecture?\n",
        "\n",
        "In machine learning, especially in NLP, a **model architecture** refers to the **blueprint or structure** of the neural network ‚Äî including:\n",
        "\n",
        "* How the data flows (e.g., left-to-right in GPT vs encoder-decoder in BERT)\n",
        "* What layers are used (e.g., attention, feedforward, normalization)\n",
        "* How those layers are connected\n",
        "* What kind of tasks it's designed to solve (e.g., text generation, classification)\n",
        "\n",
        "### üß† Common NLP Model Architectures:\n",
        "\n",
        "| Model                             | Architecture Type        | Task Focus                 |\n",
        "| --------------------------------- | ------------------------ | -------------------------- |\n",
        "| GPT (e.g., GPT-2, GPT-3, Mistral) | Decoder-only (Causal LM) | Text generation            |\n",
        "| BERT                              | Encoder-only             | Classification, NER        |\n",
        "| T5, BART                          | Encoder‚Äìdecoder          | Translation, summarization |\n",
        "| Falcon                            | Decoder-only             | Text generation            |\n",
        "| LLaMA                             | Decoder-only             | Text generation            |\n",
        "\n",
        "So when you choose a model like `tiiuae/falcon-rw-1b`, you're choosing a **decoder-only causal transformer**, optimized for left-to-right text generation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß What is a ‚ÄúBase Model (No Head)‚Äù?\n",
        "\n",
        "In transformer models, the **‚Äúhead‚Äù** is a small module added on top of the base transformer to perform a specific task.\n",
        "\n",
        "### üß© Structure:\n",
        "\n",
        "```\n",
        "[Transformer Layers] ‚Üí [Task-specific Head] ‚Üí Output\n",
        "```\n",
        "\n",
        "### üõ†Ô∏è ‚ÄúBase Model‚Äù Means:\n",
        "\n",
        "* You get **just the transformer stack**\n",
        "* No attached classification, generation, or QA head\n",
        "* It‚Äôs used for:\n",
        "\n",
        "  * Getting **embeddings**\n",
        "  * **Fine-tuning** for your own task with your own head\n",
        "  * Low-level model inspection\n",
        "\n",
        "### ‚ö†Ô∏è Why It Matters:\n",
        "\n",
        "| Model Class                          | What It Does                                                                                |\n",
        "| ------------------------------------ | ------------------------------------------------------------------------------------------- |\n",
        "| `AutoModel`                          | Returns the raw transformer ‚Äî no generation, just hidden states.                            |\n",
        "| `AutoModelForCausalLM`               | Adds a **language modeling head** for next-token prediction (used in GPT-style generation). |\n",
        "| `AutoModelForSequenceClassification` | Adds a **classification head** ‚Äî e.g., sentiment prediction.                                |\n",
        "\n",
        "---\n",
        "\n",
        "### üëá Visual Analogy\n",
        "\n",
        "| Layer              | Role                                      |\n",
        "| ------------------ | ----------------------------------------- |\n",
        "| Transformer \"Base\" | Brain ‚Äî processes input                   |\n",
        "| \"Head\"             | Mouth or hand ‚Äî outputs a specific action |\n",
        "| Base only          | Just thinking, no action                  |\n",
        "| Base + LM Head     | Thinking and speaking the next word       |\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Example Use Cases:\n",
        "\n",
        "```python\n",
        "# Just the transformer (base only, for embeddings)\n",
        "from transformers import AutoModel\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# For generating text (has LM head)\n",
        "from transformers import AutoModelForCausalLM\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Summary:\n",
        "\n",
        "* **Model architecture** = the structure that defines *how* the model processes input (e.g., GPT-style, BERT-style).\n",
        "* **Base model** = just the transformer layers; useful for feature extraction or custom tasks.\n",
        "* **Head** = task-specific layer added on top (e.g., for classification or generation).\n"
      ],
      "metadata": {
        "id": "8_x9n4PFaERP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're asking exactly the right question ‚Äî and yes, you're getting into the **core architecture reuse** in NLP models.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **Short Answer:**\n",
        "\n",
        "Yes ‚Äî **the same underlying model (\"base\") is used for both embeddings and prediction**. The only difference is whether a **task-specific head** is attached on top.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Let‚Äôs Break That Down:\n",
        "\n",
        "### üß† 1. **Base Model = Transformer Stack**\n",
        "\n",
        "* It's just layers of:\n",
        "\n",
        "  * Self-attention\n",
        "  * Feedforward layers\n",
        "  * Layer norms\n",
        "  * Positional encodings\n",
        "* **It turns input tokens into hidden representations** (aka embeddings at different levels).\n",
        "\n",
        "This is useful for:\n",
        "\n",
        "* Extracting **embeddings**\n",
        "* Using the model for **fine-tuning**\n",
        "* Passing into **your own custom head**\n",
        "\n",
        "---\n",
        "\n",
        "### üèóÔ∏è 2. **Heads = Task Modules**\n",
        "\n",
        "These are small neural layers added on top of the base, depending on your task.\n",
        "\n",
        "| Head Type                    | Use Case                        |\n",
        "| ---------------------------- | ------------------------------- |\n",
        "| Causal LM head (e.g., GPT)   | Predict next token              |\n",
        "| Sequence classification head | Sentiment, topic classification |\n",
        "| Token classification head    | Named Entity Recognition        |\n",
        "| QA head                      | Question Answering              |\n",
        "| Seq2Seq decoder head         | Translation, summarization      |\n",
        "\n",
        "> These heads often use just the **last hidden state** of the transformer to make predictions.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è 3. **So: One Base, Many Tasks**\n",
        "\n",
        "You can use **one pretrained transformer base** for:\n",
        "\n",
        "| Purpose        | Code Example                         |\n",
        "| -------------- | ------------------------------------ |\n",
        "| Embedding      | `AutoModel` (no head)                |\n",
        "| Generation     | `AutoModelForCausalLM`               |\n",
        "| Classification | `AutoModelForSequenceClassification` |\n",
        "\n",
        "The head defines the task ‚Äî not the base.\n",
        "\n",
        "---\n",
        "\n",
        "### üìå Real-World Analogy:\n",
        "\n",
        "> The base model is like a **Swiss Army knife blade** ‚Äî it does the core processing.\n",
        "> The \"head\" is like choosing the **right tool attachment**: screwdriver, scissors, bottle opener = generation, classification, QA.\n",
        "\n",
        "---\n",
        "\n",
        "### üë®‚Äçüî¨ Bonus: Can You Add Your Own Head?\n",
        "\n",
        "Yes. This is how **fine-tuning** works in practice:\n",
        "\n",
        "* Load a base model\n",
        "* Add a new head (e.g., 3-class sentiment output)\n",
        "* Train on your own labeled data\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary:\n",
        "\n",
        "* ‚úÖ Same base model ‚Üí different tasks via different heads\n",
        "* ‚úÖ Base alone = embeddings\n",
        "* ‚úÖ Base + head = predictions (text, class, etc.)\n",
        "* ‚úÖ You can switch heads or make your own\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hityBLKxa0EB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QD1vwjc-aH0L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}