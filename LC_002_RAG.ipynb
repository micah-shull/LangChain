{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMrDMAA0x+QMiezdaO0Db5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_002_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧭 Step 1: What Is RAG (Retrieval-Augmented Generation)?\n",
        "\n",
        "RAG is a system design that enhances LLM outputs by:\n",
        "\n",
        "1. **Retrieving** relevant information from an external knowledge base (like a PDF, Word doc, etc.)\n",
        "2. **Feeding that information into the prompt** sent to the LLM\n",
        "3. Letting the LLM generate a response **grounded** in retrieved facts\n",
        "\n",
        "### 🔁 Pipeline Summary:\n",
        "\n",
        "```\n",
        "User Query → Retrieve Relevant Docs → Inject into Prompt → Generate Answer\n",
        "```\n",
        "\n",
        "This solves a key problem:\n",
        "\n",
        "> LLMs don’t know about *your* business — RAG injects your domain knowledge at runtime.\n",
        "\n",
        "---\n",
        "\n",
        "## 💼 How You’ll Use It for Business\n",
        "\n",
        "With your business documents (like manuals, reports, contracts, etc.), a RAG pipeline will:\n",
        "\n",
        "* 🔎 Search your document corpus for relevant snippets\n",
        "* 💬 Feed that context into a prompt\n",
        "* 🧠 Let the LLM generate a **grounded, relevant answer**\n",
        "* 🔄 Repeat for any question about your internal knowledge\n",
        "\n",
        "For example:\n",
        "\n",
        "> ❓ *\"What is our refund policy on international orders?\"*\n",
        "> → Retrieves your company’s policy PDF\n",
        "> → Generates a clear, policy-grounded answer\n",
        "\n",
        "---\n",
        "\n",
        "## 🧰 How We'll Build It in LangChain\n",
        "\n",
        "Here’s the **LangChain-based architecture** we’ll start with:\n",
        "\n",
        "### 🧱 RAG Components in LangChain\n",
        "\n",
        "| Component          | What it does                    | LangChain Tool                                    |         |\n",
        "| ------------------ | ------------------------------- | ------------------------------------------------- | ------- |\n",
        "| 📄 Document Loader | Load your business files        | `DirectoryLoader`, `PyPDFLoader`, etc.            |         |\n",
        "| 🧹 Text Splitter   | Breaks long docs into chunks    | `RecursiveCharacterTextSplitter`                  |         |\n",
        "| 📚 Embeddings      | Turns text into vectors         | `OpenAIEmbeddings`, `HuggingFaceEmbeddings`, etc. |         |\n",
        "| 🏭 Vector Store    | Stores and searches text chunks | `FAISS`, `Chroma`, `Weaviate`, etc.               |         |\n",
        "| 🔎 Retriever       | Finds relevant docs per query   | `vectorstore.as_retriever()`                      |         |\n",
        "| 💬 Prompt          | Wraps context + question        | `ChatPromptTemplate`                              |         |\n",
        "| 🧠 LLM             | Generates answer                | `ChatOpenAI`, `HuggingFaceEndpoint`, etc.         |         |\n",
        "| 🔗 Chain           | Glues it all together           | `Runnable` chain via \\`                           | \\` pipe |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔨 Example Flow We'll Build\n",
        "\n",
        "```text\n",
        "📄 Load + chunk documents\n",
        "   ↓\n",
        "🔢 Create embeddings\n",
        "   ↓\n",
        "🧠 Store in vector database (Chroma or FAISS)\n",
        "   ↓\n",
        "🔍 At runtime:\n",
        "   → User query\n",
        "   → Retrieve relevant chunks\n",
        "   → Feed into prompt template\n",
        "   → LLM generates grounded answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🏁 Next Steps\n",
        "\n",
        "Now that you have the “what and why,” here's how we’ll start building:\n",
        "\n",
        "1. **Set up the doc loader and splitter**\n",
        "2. **Embed and store in vector DB**\n",
        "3. **Create retriever + prompt template**\n",
        "4. **Pipe: retriever → prompt → LLM → parser**\n",
        "5. **Query the chain with a real question**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MknVYhgJnCWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🧱 Plan for RAG Notebook (Step-by-Step Build)\n",
        "\n",
        "We'll build the following stages:\n",
        "\n",
        "1. **📂 Load your `.txt` documents**\n",
        "2. **🧹 Split text into chunks** (for better retrieval)\n",
        "3. **🔢 Embed chunks using Hugging Face embeddings**\n",
        "4. **📚 Store them in Chroma vector DB**\n",
        "5. **🔎 Set up retriever**\n",
        "6. **💬 Build prompt with retrieved docs + user query**\n",
        "7. **🧠 Generate response using open-source model**\n",
        "8. **✅ Pretty-print the result**\n",
        "\n"
      ],
      "metadata": {
        "id": "M6H05G_rnwoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Install Packages"
      ],
      "metadata": {
        "id": "KvNkrKlLz8JB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGONDxComnoP",
        "outputId": "abce7a3d-8ce4-4874-a644-d6b09d9979e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.9/194.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# !pip install -q langchain langchain-openai python-dotenv\n",
        "# !pip install --upgrade --quiet  langchain-huggingface text-generation transformers google-search-results numexpr langchainhub sentencepiece jinja2 bitsandbytes accelerate\n",
        "\n",
        "!pip install --upgrade --quiet langchain langchain-huggingface chromadb python-dotenv transformers accelerate sentencepiece bitsandbytes langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## 🔍 LangChain Package Breakdown for RAG\n",
        "\n",
        "---\n",
        "\n",
        "### 📁 `from langchain_community.document_loaders import TextLoader`\n",
        "\n",
        "* **What it does:**\n",
        "  Reads `.txt` files from disk and converts them into `Document` objects — LangChain’s internal format for chunks of text with metadata.\n",
        "\n",
        "* **Example:**\n",
        "  A single `.txt` file becomes:\n",
        "\n",
        "  ```python\n",
        "  Document(page_content=\"This is your text...\", metadata={\"source\": \"filename.txt\"})\n",
        "  ```\n",
        "\n",
        "* **Why it matters:**\n",
        "  RAG needs a structured format to store, retrieve, and inject chunks into LLM prompts.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔤 `from langchain.text_splitter import RecursiveCharacterTextSplitter`\n",
        "\n",
        "* **What it does:**\n",
        "  Breaks long texts into smaller chunks (like 500-character segments) while trying to split intelligently (at sentence or paragraph boundaries if possible).\n",
        "\n",
        "* **Why it matters:**\n",
        "  Most LLMs have a token limit. Breaking text into smaller overlapping chunks improves:\n",
        "\n",
        "  * Retrieval accuracy\n",
        "  * Context relevance\n",
        "  * Overall LLM output quality\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `from langchain.embeddings import HuggingFaceEmbeddings`\n",
        "\n",
        "* **What it does:**\n",
        "  Converts each chunk of text into a **vector representation** (a list of numbers that capture semantic meaning).\n",
        "\n",
        "* **Why it matters:**\n",
        "  These embeddings are what allow you to later “search” for relevant chunks using cosine similarity or other vector math.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧠 `from langchain_huggingface import HuggingFaceEndpoint`\n",
        "\n",
        "* **What it does:**\n",
        "  A wrapper that lets LangChain send prompts to a hosted model on Hugging Face (like Zephyr or Mistral) via Hugging Face Inference API.\n",
        "\n",
        "* **Why it matters:**\n",
        "  This is your LLM — the component that actually generates the response based on the prompt + retrieved context.\n",
        "\n",
        "---\n",
        "\n",
        "### 📚 `from langchain.vectorstores import Chroma`\n",
        "\n",
        "* **What it does:**\n",
        "  Stores the embeddings and allows similarity search (retrieval) later.\n",
        "\n",
        "* **Why it matters:**\n",
        "  This is your **knowledge base**. When a user asks a question, LangChain will:\n",
        "\n",
        "  1. Embed the question\n",
        "  2. Compare it to stored vectors\n",
        "  3. Retrieve the most relevant text chunks\n",
        "\n",
        "---\n",
        "\n",
        "### 💬 `from langchain_core.prompts import ChatPromptTemplate`\n",
        "\n",
        "* **What it does:**\n",
        "  Lets you define reusable, fill-in-the-blank-style prompt templates.\n",
        "\n",
        "* **Why it matters:**\n",
        "  This controls what the LLM sees — and how the retrieved documents and user question are formatted into a coherent prompt.\n",
        "\n",
        "---\n",
        "\n",
        "### 📤 `from langchain_core.output_parsers import StrOutputParser`\n",
        "\n",
        "* **What it does:**\n",
        "  After the LLM returns a result, this turns the raw output into a clean string (or list, or dict depending on what you're doing).\n",
        "\n",
        "* **Why it matters:**\n",
        "  It’s how you get **clean answers** instead of messy LLM responses.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔗 `from langchain_core.runnables import Runnable`\n",
        "\n",
        "* **What it does:**\n",
        "  Provides the ability to chain steps together using the `|` (pipe) syntax.\n",
        "\n",
        "* **Why it matters:**\n",
        "  This is how you connect:\n",
        "\n",
        "  ```\n",
        "  retriever → prompt → LLM → parser\n",
        "  ```\n",
        "\n",
        "  Into a clean, readable, and reusable pipeline.\n",
        "\n",
        "---\n",
        "\n",
        "### 🧾 `import textwrap`\n",
        "\n",
        "* **What it does:**\n",
        "  Nicely formats long text outputs in the notebook by wrapping lines to fit the screen.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Summary\n",
        "\n",
        "| Category        | Package                          | Role                                |\n",
        "| --------------- | -------------------------------- | ----------------------------------- |\n",
        "| 📂 Load Text    | `TextLoader`                     | Loads `.txt` files                  |\n",
        "| 🧹 Preprocess   | `RecursiveCharacterTextSplitter` | Chunks text                         |\n",
        "| 🔢 Vectorize    | `HuggingFaceEmbeddings`          | Turns chunks into vectors           |\n",
        "| 🧠 LLM          | `HuggingFaceEndpoint`            | Calls an open-source LLM            |\n",
        "| 📚 Store/Search | `Chroma`                         | Stores and retrieves chunks         |\n",
        "| 💬 Prompt       | `ChatPromptTemplate`             | Formats the final LLM prompt        |\n",
        "| 📤 Parse        | `StrOutputParser`                | Cleans up LLM output                |\n",
        "| 🔗 Chain        | `Runnable`                       | Combines components into a pipeline |\n",
        "\n"
      ],
      "metadata": {
        "id": "39mq6vJvq4Uc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Libraries"
      ],
      "metadata": {
        "id": "uoyll4oB0ANY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 🌿 Environment\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import langchain; print(langchain.__version__)\n",
        "\n",
        "# 📄 Document loading + text splitting\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# 🔢 Embeddings + vector store\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# 🧠 Open-source LLM (Zephyr via Hugging Face Inference)\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "# 💬 Prompt & output\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 🔗 Chaining\n",
        "from langchain_core.runnables import Runnable\n",
        "\n",
        "# 🧾 Pretty output\n",
        "import textwrap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q58WFbWobSR",
        "outputId": "56fdfadf-c62d-4075-8911-2f459c7609b1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import getpass\n",
        "# import os\n",
        "\n",
        "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "id": "UC-gfziS2ivG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ✅ Step-by-Step Plan for Document Loading & Cleaning\n",
        "\n",
        "### 🧾 1. **Load the `.txt` files**\n",
        "\n",
        "We’ll loop through all files in the folder using `TextLoader`.\n",
        "\n",
        "### 🧹 2. **Optional Cleaning**\n",
        "\n",
        "Basic cleaning (e.g. stripping newlines, extra whitespace) is often helpful **before splitting**, especially if the files came from exports or copy-paste.\n",
        "\n",
        "### ✂️ 3. **Split into chunks**\n",
        "\n",
        "We’ll use `RecursiveCharacterTextSplitter` to chunk documents (typically 500–1000 characters with slight overlap for context continuity).\n",
        "\n",
        "---\n",
        "\n",
        "### 🧼 Why Basic Cleaning Helps\n",
        "\n",
        "* Removes linebreaks and blank lines that confuse LLMs\n",
        "* Avoids splitting chunks in weird places\n",
        "* Standardizes format before embedding\n",
        "\n",
        "Later you can add more advanced cleaning (e.g., remove boilerplate, normalize headers), but this is a solid default.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmSqcFpbs5Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from pprint import pprint\n",
        "\n",
        "# Path to your documents\n",
        "docs_path = \"/content/sample_data/CFFC_docs\"\n",
        "\n",
        "# Step 1: Load all .txt files in the folder\n",
        "raw_documents = []\n",
        "for filename in os.listdir(docs_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(docs_path, filename)\n",
        "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        docs = loader.load()\n",
        "        raw_documents.extend(docs)\n",
        "\n",
        "print(f\"Loaded {len(raw_documents)} documents.\")\n",
        "\n",
        "# Step 2 (optional): Clean up newlines and extra whitespace\n",
        "def clean_doc(doc: Document) -> Document:\n",
        "    cleaned = \" \".join(doc.page_content.split())  # Removes newlines & extra spaces\n",
        "    return Document(page_content=cleaned, metadata=doc.metadata)\n",
        "\n",
        "cleaned_documents = [clean_doc(doc) for doc in raw_documents]\n",
        "\n",
        "# Step 3: Split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=75\n",
        ")\n",
        "\n",
        "chunked_documents = splitter.split_documents(cleaned_documents)\n",
        "\n",
        "print(f\"Split into {len(chunked_documents)} total chunks.\")\n",
        "\n",
        "# Preview the first 5 chunks\n",
        "print(f\"Showing first 5 of {len(chunked_documents)} chunks:\\n\")\n",
        "\n",
        "for i, doc in enumerate(chunked_documents[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
        "    print(textwrap.fill(doc.page_content[:500], width=100))  # limit preview to 500 characters\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdAHICO-sYkc",
        "outputId": "35971a4a-8e9f-4be9-ac12-c2e423a73a09"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 7 documents.\n",
            "Split into 64 total chunks.\n",
            "Showing first 5 of 64 chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: /content/sample_data/CFFC_docs/CFFC_Gainesville Economic Indicators That Matter to Local Businesses.txt\n",
            "\n",
            "Cashflow 4Cast Gainesville Economic Indicators That Matter to Local Businesses on April 02, 2025 📗\n",
            "Gainesville Economic Indicators That Matter to Local Businesses 1. Average Weekly Earnings\n",
            "(Gainesville) What It Is: This tracks the average amount workers in Gainesville earn per week —\n",
            "across all private sector jobs. It’s one of the clearest measures of take-home pay and gives insight\n",
            "into what people can realistically afford. Why It Matters for Gainesville: When earnings drop,\n",
            "households tighten\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: /content/sample_data/CFFC_docs/CFFC_Gainesville Economic Indicators That Matter to Local Businesses.txt\n",
            "\n",
            "Why It Matters for Gainesville: When earnings drop, households tighten their budgets. That means\n",
            "fewer nights out, postponed purchases, and more cautious spending habits — all of which affect local\n",
            "businesses directly. 💬 Local Insight Since December 1, 2024, average weekly earnings in Gainesville\n",
            "have fallen from $1,096.83 to $1,080.16, a 1.52% decline. That shift may seem minor, but for many\n",
            "local households: It’s the difference between going out or staying in It reduces spending on\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: /content/sample_data/CFFC_docs/CFFC_Gainesville Economic Indicators That Matter to Local Businesses.txt\n",
            "\n",
            "It’s the difference between going out or staying in It reduces spending on discretionary items It\n",
            "adds pressure to manage rising living costs For local businesses, this signals a subtle but\n",
            "important change in customer purchasing power — even if foot traffic remains steady, the average\n",
            "spend per customer may start to drop. Gainesville Average Weekly Earnings 2. Retail Trade Employment\n",
            "(Gainesville) What It Is: This tracks how many people are working in Gainesville’s retail sector —\n",
            "stores,\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Source: /content/sample_data/CFFC_docs/CFFC_Gainesville Economic Indicators That Matter to Local Businesses.txt\n",
            "\n",
            "how many people are working in Gainesville’s retail sector — stores, shops, and service counters.\n",
            "It’s a strong proxy for both consumer demand and small business health. Why It Matters for\n",
            "Gainesville: When retail employment drops, it may signal falling customer demand, reduced store\n",
            "hours, or even closures. For many businesses, it’s also a reflection of how confident owners are in\n",
            "the near future. 💬 Local Insight Since December 1, 2024, Gainesville’s retail employment has\n",
            "declined from 16.9K\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Source: /content/sample_data/CFFC_docs/CFFC_Gainesville Economic Indicators That Matter to Local Businesses.txt\n",
            "\n",
            "December 1, 2024, Gainesville’s retail employment has declined from 16.9K to 16.5K, a 2.37%\n",
            "decrease. That may seem small, but it suggests businesses are: Pulling back on staffing Bracing for\n",
            "softer sales Trying to manage rising costs or slower traffic If you're feeling pressure on both\n",
            "sales and payroll, this trend helps explain why — it's not just you. Gainesville Retail Trade\n",
            "Employment 3. Gainesville Unemployment Rate What It Is: The percentage of Gainesville’s labor force\n",
            "actively seeking\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 🏢 Where Businesses Store Chunked Data in Production\n",
        "\n",
        "In production, storing your chunked + embedded data is **crucial** for:\n",
        "\n",
        "* 🔄 Avoiding repeated computation\n",
        "* 🔍 Fast retrieval\n",
        "* 🧩 Scaling to large document sets\n",
        "* 🧠 Supporting updates or reindexing\n",
        "\n",
        "### ✅ Common Storage Options\n",
        "\n",
        "| Option                            | Description                                       | Used When                                         |\n",
        "| --------------------------------- | ------------------------------------------------- | ------------------------------------------------- |\n",
        "| **Chroma (on-disk)**              | Simple embedded DB, fast, file-based              | Local or lightweight deployments                  |\n",
        "| **FAISS (with disk persistence)** | High-performance vector index                     | When doing custom similarity search, scaling up   |\n",
        "| **Qdrant**                        | Open-source vector DB with REST API               | When you need a full DB server or remote access   |\n",
        "| **Weaviate**                      | Scalable DB with hybrid search (vector + keyword) | For enterprise-grade RAG APIs                     |\n",
        "| **Pinecone**                      | Hosted vector DB (SaaS)                           | Fully managed, production-grade retrieval         |\n",
        "| **PostgreSQL + pgvector**         | SQL DB with vector support                        | When combining metadata + retrieval in one system |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔐 Key Business Considerations\n",
        "\n",
        "| Factor                 | Why it matters                                                 |\n",
        "| ---------------------- | -------------------------------------------------------------- |\n",
        "| **Persistence**        | You don’t want to re-chunk & re-embed every time your app runs |\n",
        "| **Scalability**        | If your corpus grows, can the DB handle 100K+ chunks?          |\n",
        "| **Search performance** | Sub-second vector search time is crucial for fast answers      |\n",
        "| **Security**           | Internal document corpora need access control                  |\n",
        "| **Deployment**         | On-prem vs cloud, latency, compliance, etc.                    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_X5f_a-EuDam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## ✅ Step: Embed + Persist in Chroma\n",
        "\n",
        "We’ll use:\n",
        "\n",
        "* `HuggingFaceEmbeddings` to convert text to vectors\n",
        "* `Chroma` to store those vectors on disk so you don’t have to recompute\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVMiFfU7ulqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Step 1: Set up Hugging Face embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Step 2: Set up Chroma with persistence\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "print(f\"✅ Stored {len(chunked_documents)} chunks in Chroma at '{persist_dir}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EJR3GwqtXwu",
        "outputId": "c2e2c7e5-babf-46bb-aa4d-53c29023a3f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stored 64 chunks in Chroma at 'chroma_db'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧠 Next Step: Retrieval + Generation (the \"G\" in RAG)\n",
        "\n",
        "We’ll now:\n",
        "\n",
        "1. 🔍 **Create a retriever** from your Chroma vectorstore\n",
        "2. 🧾 **Build a prompt** that includes:\n",
        "\n",
        "   * Retrieved chunks (context)\n",
        "   * A user query\n",
        "3. 🧠 **Send it to your LLM (Zephyr)**\n",
        "4. ✅ **Get grounded answers** based on your document corpus"
      ],
      "metadata": {
        "id": "bYu3JSZyweG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 1: Create the Retriever\n",
        "\n",
        "\n",
        "## 🔍 What `k` Controls\n",
        "\n",
        "```python\n",
        "retriever.search_kwargs = {\"k\": 4}\n",
        "```\n",
        "\n",
        "This tells your vector store to return the **top `k` most relevant document chunks** when you ask a question.\n",
        "\n",
        "For example:\n",
        "\n",
        "* If `k = 1`: You get only the single most relevant chunk.\n",
        "* If `k = 4`: You get the top 4 chunks (in descending order of similarity to your query).\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Benefits of **Increasing** `k`\n",
        "\n",
        "| Benefit                         | Explanation                                                                |\n",
        "| ------------------------------- | -------------------------------------------------------------------------- |\n",
        "| 🧠 **More context**             | Gives the LLM more information to base its answer on.                      |\n",
        "| 🧩 **Better coverage**          | If relevant info is spread across several chunks, this helps catch it all. |\n",
        "| 🚫 **Reduces hallucination**    | More context = less guessing from the model.                               |\n",
        "| 📚 **Supports complex queries** | Especially useful when answering multi-part or nuanced questions.          |\n",
        "\n",
        "---\n",
        "\n",
        "## ⚠️ Tradeoffs of **Too Large** `k`\n",
        "\n",
        "| Issue                     | Explanation                                                      |\n",
        "| ------------------------- | ---------------------------------------------------------------- |\n",
        "| 🌀 **Longer prompts**     | LLMs have a token limit. Too much context = prompt truncation.   |\n",
        "| 😕 **Diluted relevance**  | Later chunks may be less relevant, and can “distract” the model. |\n",
        "| 🐢 **Slower performance** | More tokens = more cost + longer generation time.                |\n",
        "\n",
        "---\n",
        "\n",
        "## 🔧 Good Starting Defaults\n",
        "\n",
        "| Use Case                | Suggested `k`                                       |\n",
        "| ----------------------- | --------------------------------------------------- |\n",
        "| 🔍 Simple Q\\&A          | `k = 2–4`                                           |\n",
        "| 📚 Summary or synthesis | `k = 4–6`                                           |\n",
        "| 🧪 Experimental tuning  | Try `k = 1`, `3`, `5`, `10` and compare LLM answers |\n",
        "\n",
        "You can even dynamically tune it based on query type — e.g., use higher `k` for vague or multi-part questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "fWnzZXxuwv0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#  customize search depth with:\n",
        "retriever.search_kwargs = {\"k\": 3}  # retrieve top 4 relevant chunks"
      ],
      "metadata": {
        "id": "rY_aJ08FwiX7"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 2: Create the Prompt Template\n",
        "\n",
        "We’ll create a structured prompt like this:\n",
        "\n",
        "> \"Use the following context to answer the question.\n",
        ">\n",
        "> Context:\n",
        "> {context}\n",
        ">\n",
        "> Question: {question}\n",
        ">\n",
        "> Helpful Answer:\"\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wYJTRiY_wXDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that uses business documents to answer questions.\n",
        "Use the following context to answer the question as accurately as possible.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "pPqXCKxcwcNU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 3: Create the RAG Chain"
      ],
      "metadata": {
        "id": "89iAu-wbxB8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Load token from .env.\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Set up LLM\n",
        "llm_HF = HuggingFaceEndpoint(\n",
        "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # or HF_TOKEN if you renamed it\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm_HF)\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\"question\": d[\"question\"], \"docs\": retriever.invoke(d[\"question\"])})\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "yhoa2RrnzUw3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 4: Run a Query!"
      ],
      "metadata": {
        "id": "q56uT-29xJZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "import textwrap\n",
        "print(\"\\n\" + textwrap.fill(response, width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljnbnq2JxKKZ",
        "outputId": "51ce7644-1fbe-49f2-86a0-873fa0c78fe6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "According to Cashflow 4Cast's Federal Economic Indicators report for April 2, 2025, there are two\n",
            "economic indicators that are impacting local businesses in Gainesville: inflation and unemployment\n",
            "rate. Inflation, as measured by the Consumer Price Index (CPI), has been on the rise, with prices\n",
            "for everyday goods and services increasing. This may lead to decreased consumer demand and\n",
            "businesses rethinking hiring plans. The recent increase in the unemployment rate in Gainesville,\n",
            "from 3.3% in December 2024 to 4.1% as of April 2, 2025, is also affecting local businesses. This\n",
            "higher unemployment rate may result in fewer walk-in customers or clients, increased price\n",
            "sensitivity among shoppers, and less confidence in hiring or expansion plans. These economic\n",
            "indicators indicate that local businesses may need to adjust their strategies to adapt to the\n",
            "changing economic climate.\n"
          ]
        }
      ]
    }
  ]
}