{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPnMu+UH/fpa2kysaXp/k7b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_000.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**LangChain**\n",
        "LangChain is a powerful framework designed to simplify building applications that use **large language models (LLMs)**. It's especially useful for tasks like:\n",
        "\n",
        "* Question answering over documents\n",
        "* Chatbots\n",
        "* Code analysis\n",
        "* Agent-based workflows\n",
        "* Integration with tools like search engines, APIs, and databases\n",
        "\n",
        "---\n",
        "\n",
        "### üîß What You Need to Know First\n",
        "\n",
        "Before diving into LangChain, it's helpful to have a basic understanding of:\n",
        "\n",
        "* **Python programming**\n",
        "* How **LLMs** like OpenAI's GPT work at a high level\n",
        "* Familiarity with APIs and JSON\n",
        "* Optional: Experience with libraries like `pydantic`, `FastAPI`, or `Transformers`\n",
        "\n",
        "---\n",
        "\n",
        "### üß± LangChain Core Concepts\n",
        "\n",
        "Here are the main building blocks of LangChain:\n",
        "\n",
        "| Concept              | Description                                              |\n",
        "| -------------------- | -------------------------------------------------------- |\n",
        "| **LLMs**             | Wraps models like OpenAI, Cohere, etc.                   |\n",
        "| **Prompts**          | Templates that define the input to the LLM               |\n",
        "| **Chains**           | Sequences of calls (e.g., prompt ‚Üí model ‚Üí output)       |\n",
        "| **Agents**           | Decision-making systems that use tools to complete tasks |\n",
        "| **Tools**            | Functions the agent can call (e.g., search, calculator)  |\n",
        "| **Memory**           | Keeps track of the conversation history                  |\n",
        "| **Document Loaders** | Load documents from PDFs, web pages, etc.                |\n",
        "| **Vector Stores**    | Store document embeddings for semantic search            |\n",
        "| **Retrievers**       | Find relevant documents based on a query                 |\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ How to Start Learning\n",
        "\n",
        "Here‚Äôs a learning plan we can follow. Let me know your pace or focus area:\n",
        "\n",
        "#### ‚úÖ Step 1: Installation & Setup\n",
        "\n",
        "* Install: `pip install langchain openai`\n",
        "* Get OpenAI API key (or use other model providers)\n",
        "\n",
        "#### ‚úÖ Step 2: Your First Chain\n",
        "\n",
        "* Build a simple prompt ‚Üí LLM ‚Üí response chain\n",
        "\n",
        "#### ‚úÖ Step 3: Use Documents\n",
        "\n",
        "* Load documents (e.g., PDF, text)\n",
        "* Embed them using `FAISS` or `Chroma`\n",
        "* Create a retriever and ask questions\n",
        "\n",
        "#### ‚úÖ Step 4: Build an Agent\n",
        "\n",
        "* Give it tools (search, math, retrieval)\n",
        "* Use ReAct pattern for reasoning\n",
        "\n",
        "#### ‚úÖ Step 5: Add Memory\n",
        "\n",
        "* Add chat memory to track conversations\n",
        "\n",
        "#### ‚úÖ Step 6: Build a Full App\n",
        "\n",
        "* Combine chains, agents, and retrievers into a useful app\n",
        "* Possibly deploy with Streamlit or FastAPI\n",
        "\n"
      ],
      "metadata": {
        "id": "fGSOShMa22HA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Information"
      ],
      "metadata": {
        "id": "UHAt1xQQ3dnP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade \\\n",
        "#     langchain \\\n",
        "#     langchain-community \\\n",
        "#     langchain-core \\\n",
        "#     openai \\\n",
        "#     chromadb \\\n",
        "#     faiss-cpu \\\n",
        "#     tiktoken\n",
        "\n",
        "# !pip install langchain-openai"
      ],
      "metadata": {
        "id": "1Ydl_Do14D-N"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXTc0nLe2mDw",
        "outputId": "c3723d56-876d-4c95-ee86-bc99c9bf39db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain: 0.3.25\n",
            "openai: 1.82.1\n",
            "chromadb: 1.0.12\n",
            "faiss-cpu: 1.11.0\n",
            "faiss-gpu: NOT INSTALLED\n",
            "tiktoken: 0.9.0\n",
            "pydantic: 2.11.4\n",
            "numpy: 2.0.2\n",
            "pandas: 2.2.2\n",
            "requests: 2.32.3\n",
            "httpx: 0.28.1\n",
            "tenacity: 9.1.2\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import pkg_resources\n",
        "\n",
        "# List of relevant packages\n",
        "packages = [\n",
        "    \"langchain\",\n",
        "    \"openai\",\n",
        "    \"chromadb\",\n",
        "    \"faiss-cpu\",\n",
        "    \"faiss-gpu\",\n",
        "    \"tiktoken\",\n",
        "    \"pydantic\",\n",
        "    \"numpy\",\n",
        "    \"pandas\",\n",
        "    \"requests\",\n",
        "    \"httpx\",\n",
        "    \"tenacity\"\n",
        "]\n",
        "\n",
        "def check_versions(packages):\n",
        "    for pkg in packages:\n",
        "        try:\n",
        "            version = pkg_resources.get_distribution(pkg).version\n",
        "            print(f\"{pkg}: {version}\")\n",
        "        except pkg_resources.DistributionNotFound:\n",
        "            print(f\"{pkg}: NOT INSTALLED\")\n",
        "\n",
        "check_versions(packages)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load .env file\n",
        "load_dotenv(\"/content/API_KEYS.env\")\n",
        "\n",
        "# Now it's available as an environment variable\n",
        "print(\"API Key loaded:\", os.getenv(\"OPENAI_API_KEY\")[:8] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kL-ioXO84j5m",
        "outputId": "5c43b977-6cda-4288-9d71-686d5e4d5fa9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API Key loaded: sk-proj-...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up the LLM (you'll be prompted for your API key if not set)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Build a prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Chain it together\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run it\n",
        "response = chain.invoke({\"topic\": \"AI and bananas\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8aEvk8p3j48",
        "outputId": "abaffe10-333c-4305-e105-675f7f8bf0b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI break up with the banana? Because it couldn't handle its a-peeling personality!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß© Step 1: `ChatPromptTemplate` ‚Äî What Is It and Why Use It?\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **What Is a Prompt Template?**\n",
        "\n",
        "A **prompt template** in LangChain is a **reusable blueprint** for how you structure your input to the LLM. Instead of hardcoding a full prompt like:\n",
        "\n",
        "```python\n",
        "\"Tell me a joke about AI and bananas\"\n",
        "```\n",
        "\n",
        "You create a **template with variables**, like:\n",
        "\n",
        "```python\n",
        "\"Tell me a joke about {topic}\"\n",
        "```\n",
        "\n",
        "This lets you **plug in different values** (`\"AI and bananas\"`, `\"cats\"`, `\"quantum physics\"`, etc.) at runtime.\n",
        "\n",
        "---\n",
        "\n",
        "### üí¨ `ChatPromptTemplate` vs `PromptTemplate`\n",
        "\n",
        "There are two main types:\n",
        "\n",
        "* `PromptTemplate`: for **text-completion** models (like `text-davinci-003`)\n",
        "* `ChatPromptTemplate`: for **chat models** like `gpt-3.5-turbo`, `gpt-4`\n",
        "\n",
        "Since OpenAI chat models expect structured **messages** (system, user, assistant), we use `ChatPromptTemplate`.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† How It Works\n",
        "\n",
        "This line:\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "```\n",
        "\n",
        "does the following:\n",
        "\n",
        "| Line                                                    | Action                                                     |\n",
        "| ------------------------------------------------------- | ---------------------------------------------------------- |\n",
        "| `from langchain_core.prompts import ChatPromptTemplate` | Imports the class from the new LangChain structure         |\n",
        "| `ChatPromptTemplate.from_template(...)`                 | Creates a chat-style prompt from a single user message     |\n",
        "| `\"Tell me a joke about {topic}\"`                        | Defines the **template string**, with a variable `{topic}` |\n",
        "\n",
        "Later, when you run:\n",
        "\n",
        "```python\n",
        "response = chain.invoke({\"topic\": \"AI and bananas\"})\n",
        "```\n",
        "\n",
        "LangChain replaces `{topic}` with `\"AI and bananas\"` and sends this to the LLM as a chat message from the \"user\".\n",
        "\n"
      ],
      "metadata": {
        "id": "nJjC1oi56W-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**ChatPromptTemplate**"
      ],
      "metadata": {
        "id": "mrq2ThhL9zsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üß† 1. Simple Template with One Variable\n",
        "prompt = ChatPromptTemplate.from_template(\"Explain the concept of {topic} in simple terms.\")\n",
        "prompt.invoke({\"topic\": \"black holes\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "esc8y2V26aWP",
        "outputId": "efa85034-8de5-41fc-dd99-b10c77f5c2b0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Explain the concept of black holes in simple terms.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üíº 2. Professional Email Generator\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a professional email to {recipient_name} about {topic}, and make it sound polite and concise.\"\n",
        ")\n",
        "prompt.invoke({\n",
        "    \"recipient_name\": \"Dr. Martinez\",\n",
        "    \"topic\": \"delayed project timeline\"\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjelexe27DgY",
        "outputId": "cb1bcae9-7f35-440a-b740-15aa0fc9e58c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Write a professional email to Dr. Martinez about delayed project timeline, and make it sound polite and concise.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úçÔ∏è 3. Story Starter Prompt\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a short story about a {character_type} who discovers a {mystical_object} in the {setting}.\"\n",
        ")\n",
        "prompt.invoke({\n",
        "    \"character_type\": \"young inventor\",\n",
        "    \"mystical_object\": \"talking crystal\",\n",
        "    \"setting\": \"abandoned laboratory\"\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YmnkFJh7POk",
        "outputId": "25c3a138-2b25-40a1-ce94-30ec9a1c3546"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='Write a short story about a young inventor who discovers a talking crystal in the abandoned laboratory.', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üßë‚Äçüíº 4. Customer Support Bot\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are a helpful customer support agent. A customer says: '{customer_message}'. How would you respond?\"\n",
        ")\n",
        "prompt.invoke({\n",
        "    \"customer_message\": \"I was charged twice for my subscription.\"\n",
        "})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_un4jiCw7PMO",
        "outputId": "6152c20b-ad4f-4b96-be05-c198b005796f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content=\"You are a helpful customer support agent. A customer says: 'I was charged twice for my subscription.'. How would you respond?\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üóÇ 5. System + User Messages\n",
        "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a wise science tutor.\"),\n",
        "    (\"human\", \"Can you explain {concept} like I'm five?\")\n",
        "])\n",
        "\n",
        "prompt.invoke({\"concept\": \"photosynthesis\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmYXKsDQ7PJq",
        "outputId": "65f9a9fe-dcc9-45f4-d708-5f538ba36f77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are a wise science tutor.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Can you explain photosynthesis like I'm five?\", additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## üß© What‚Äôs Going On in This Example?\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "```\n",
        "\n",
        "### üîç Breakdown:\n",
        "\n",
        "You‚Äôre creating a **multi-message prompt**, with **different roles**:\n",
        "\n",
        "* `system`: sets the behavior of the AI\n",
        "* `human`: simulates user input\n",
        "* `ai`: simulates the AI's response\n",
        "* `human`: gives another user message\n",
        "\n",
        "So you're essentially **building up a conversation** that already has history before the real user input comes in.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why Use This?\n",
        "\n",
        "This is useful when:\n",
        "\n",
        "* You want to **set context** for the model (e.g., name, tone)\n",
        "* You want the model to **see a conversation history** for better coherence\n",
        "* You‚Äôre mimicking a real dialogue in a chatbot\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ The `{}` Braces Are Placeholders\n",
        "\n",
        "```python\n",
        "(\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "...\n",
        "(\"human\", \"{user_input}\")\n",
        "```\n",
        "\n",
        "These get filled in when you call `.invoke(...)`, like so:\n",
        "\n",
        "```python\n",
        "prompt_value = template.invoke({\n",
        "    \"name\": \"Bob\",\n",
        "    \"user_input\": \"What is your name?\"\n",
        "})\n",
        "```\n",
        "\n",
        "So the result is a structured message list like:\n",
        "\n",
        "```python\n",
        "[\n",
        "  SystemMessage(\"You are a helpful AI bot. Your name is Bob.\"),\n",
        "  HumanMessage(\"Hello, how are you doing?\"),\n",
        "  AIMessage(\"I'm doing well, thanks!\"),\n",
        "  HumanMessage(\"What is your name?\")\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Key Concept: `ChatPromptValue`\n",
        "\n",
        "The `.invoke()` method returns a `ChatPromptValue`, which is just a structured object holding the **message sequence**. This is what gets passed into the `ChatOpenAI` model.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† What You Should Learn From This\n",
        "\n",
        "* You can build **multi-turn conversations** as input to LLMs.\n",
        "* `ChatPromptTemplate` accepts a **list of message tuples** like `(\"role\", \"message\")`.\n",
        "* You can still use **variables** (`{}` placeholders) in any message.\n",
        "* This approach gives the model **memory of prior messages** without needing a full memory object (more on that later).\n"
      ],
      "metadata": {
        "id": "Y4JrHdC0-nkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])"
      ],
      "metadata": {
        "id": "mqQyyP4e7PHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ü§ñ Does `ChatPromptTemplate` Store the Entire Conversation?\n",
        "\n",
        "### üî¥ Short Answer: **No, it does not *store* the conversation history** ‚Äî it just *constructs* it at that moment.\n",
        "\n",
        "What you‚Äôre doing in this example:\n",
        "\n",
        "```python\n",
        "template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
        "    (\"human\", \"Hello, how are you doing?\"),\n",
        "    (\"ai\", \"I'm doing well, thanks!\"),\n",
        "    (\"human\", \"{user_input}\"),\n",
        "])\n",
        "```\n",
        "\n",
        "...is **manually assembling a conversation history** to simulate previous exchanges. This is **static and hardcoded**, not dynamic.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† So When Does LangChain Actually *Store* Conversation History?\n",
        "\n",
        "To maintain a *real* running conversation over time (like a chatbot), LangChain provides **memory objects**.\n",
        "\n",
        "#### ‚úÖ Example: `ConversationBufferMemory`\n",
        "\n",
        "```python\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "```\n",
        "\n",
        "This stores messages dynamically as the chat continues.\n",
        "\n",
        "You then use it in a chain like:\n",
        "\n",
        "```python\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Start chatting\n",
        "response = conversation.predict(input=\"Hi, who won the World Cup in 2018?\")\n",
        "```\n",
        "\n",
        "Now, LangChain automatically appends your inputs/outputs to the memory and uses them in future context windows.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary: Manual vs. Automatic History\n",
        "\n",
        "| Method                     | Stores Conversation? | How?                      |\n",
        "| -------------------------- | -------------------- | ------------------------- |\n",
        "| `ChatPromptTemplate`       | ‚ùå No                 | Static construction       |\n",
        "| `ConversationBufferMemory` | ‚úÖ Yes                | Automatic message storage |\n",
        "| `ChatOpenAI` alone         | ‚ùå No                 | Stateless unless combined |\n",
        "\n"
      ],
      "metadata": {
        "id": "3Fe5qPmx_F8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ChatPromptTemplate` is **one of the most fundamental building blocks** in LangChain. It's worth getting solid on it before we move on. Here's a final quick-hit summary and a few **advanced tips** so you‚Äôre fully equipped:\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What You Should Know Before Moving On\n",
        "\n",
        "### ‚úÖ 1. **It Structures Input for Chat Models**\n",
        "\n",
        "* It builds the list of messages (`system`, `user`, `assistant`) that OpenAI (or similar) models expect\n",
        "* Supports **placeholders** for dynamic content (`{topic}`, `{user_input}`, `{conversation}`)\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **It‚Äôs Reusable and Parameterized**\n",
        "\n",
        "You define the structure **once**, then `invoke()` it many times with different values. Great for apps, APIs, or workflows.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Supports Message Roles and Placeholders**\n",
        "\n",
        "You can build templates with:\n",
        "\n",
        "```python\n",
        "[\n",
        "  (\"system\", \"You are...\"),\n",
        "  (\"human\", \"{user_input}\"),\n",
        "  (\"ai\", \"{response}\"),\n",
        "  (\"placeholder\", \"{conversation}\")\n",
        "]\n",
        "```\n",
        "\n",
        "You can also use:\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "```\n",
        "\n",
        "...for more explicit control.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 4. **Returns a `ChatPromptValue`**\n",
        "\n",
        "Which holds the list of structured messages ‚Äî ready to be passed to an LLM like `ChatOpenAI`.\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Advanced Tips (You‚Äôll Encounter These Soon)\n",
        "\n",
        "### üîÑ Supports Jinja2 Templates\n",
        "\n",
        "You can use more advanced formatting:\n",
        "\n",
        "```python\n",
        "ChatPromptTemplate.from_template(\n",
        "    \"Hello {{ name }}! You have {{ messages|length }} new messages.\"\n",
        ")\n",
        "```\n",
        "\n",
        "This uses [Jinja2](https://jinja.palletsprojects.com/) under the hood if you enable it.\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Works Seamlessly With:\n",
        "\n",
        "* `ChatOpenAI` or other chat model classes\n",
        "* `Memory` (like `ConversationBufferMemory`)\n",
        "* `Retrieval-Augmented Generation` (e.g., injecting context from search)\n",
        "\n",
        "---\n",
        "\n",
        "### üß± Common Pattern You‚Äôll See:\n",
        "\n",
        "```python\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "```\n",
        "\n",
        "This lets you:\n",
        "\n",
        "* Set a role\n",
        "* Maintain memory (`chat_history`)\n",
        "* Handle the newest input\n",
        "\n",
        "This structure powers **most chatbots** built in LangChain.\n",
        "\n",
        "---\n",
        "\n",
        "## üßæ TL;DR: What to Remember\n",
        "\n",
        "| Thing                | Why It Matters                                          |\n",
        "| -------------------- | ------------------------------------------------------- |\n",
        "| `ChatPromptTemplate` | The blueprint for structured prompts                    |\n",
        "| Placeholders         | Add dynamic input (e.g., `{topic}` or `{conversation}`) |\n",
        "| `invoke()`           | Fills the template with values                          |\n",
        "| Output               | A list of chat messages sent to LLMs                    |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MWIWInVoD2nP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Understanding the difference between `ChatPromptTemplate` and `ChatMessagePromptTemplate` helps you write more **modular, flexible prompt code**.\n",
        "\n",
        "\n",
        "## üß± `ChatPromptTemplate`\n",
        "\n",
        "* **High-level container** for an entire chat prompt.\n",
        "* Represents a **sequence of messages** (system, human, AI, etc.)\n",
        "* Used to **combine** multiple `ChatMessagePromptTemplate`s or message strings with roles.\n",
        "\n",
        "Think of this as the **whole chat history** structure you send to the LLM.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ `ChatMessagePromptTemplate`\n",
        "\n",
        "* **Low-level building block** representing **one message** in a chat.\n",
        "* Used when you want **fine-grained control** over each message.\n",
        "* You define the role and the template for that one message.\n",
        "\n",
        "Think of this as **a single line** in the conversation.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Analogy\n",
        "\n",
        "Imagine you're writing a play:\n",
        "\n",
        "* **`ChatMessagePromptTemplate`** is a single **line of dialogue** (with a speaker).\n",
        "* **`ChatPromptTemplate`** is the **entire script** ‚Äî a sequence of those lines.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example to Show the Difference\n",
        "\n",
        "### üß© Using `ChatPromptTemplate` with Plain Tuples (shorthand):\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant.\"),\n",
        "    (\"human\", \"What is the weather in {city}?\")\n",
        "])\n",
        "```\n",
        "\n",
        "### üß© Same Thing, But Using `ChatMessagePromptTemplate`:\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    ChatMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "system_msg = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant.\")\n",
        "human_msg = HumanMessagePromptTemplate.from_template(\"What is the weather in {city}?\")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([system_msg, human_msg])\n",
        "```\n",
        "\n",
        "Result is the same ‚Äî but this approach lets you **construct or manipulate each message** independently.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† When to Use Each\n",
        "\n",
        "| Situation                          | Use `ChatPromptTemplate` | Use `ChatMessagePromptTemplate` |\n",
        "| ---------------------------------- | ------------------------ | ------------------------------- |\n",
        "| Quick prototype                    | ‚úÖ                        | üö´                              |\n",
        "| Full chatbot                       | ‚úÖ                        | ‚úÖ (to build message blocks)     |\n",
        "| Reuse specific message parts       | üö´                       | ‚úÖ                               |\n",
        "| Need message-level logic           | üö´                       | ‚úÖ                               |\n",
        "| You like cleaner high-level syntax | ‚úÖ                        | üö´                              |\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "| Object                      | What It Represents                      | Used For                     |\n",
        "| --------------------------- | --------------------------------------- | ---------------------------- |\n",
        "| `ChatPromptTemplate`        | The full chat prompt (all messages)     | Final LLM input              |\n",
        "| `ChatMessagePromptTemplate` | One message template (system, user, ai) | Building reusable components |\n",
        "\n"
      ],
      "metadata": {
        "id": "yqb-ImduEv6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Set up the LLM (you'll be prompted for your API key if not set)\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
        "\n",
        "# Build a prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Chain it together\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# Run it\n",
        "response = chain.invoke({\"topic\": \"AI and bananas\"})\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qBuyMjUF5BS",
        "outputId": "0a968826-a0cc-4806-9b2a-9f0e68f489b4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why did the AI go bananas? Because it couldn't find its peelings!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† What Is `StrOutputParser`?\n",
        "\n",
        "### üîπ `StrOutputParser` is a simple output parser\n",
        "\n",
        "It takes the LLM's **raw output** (usually in a structured format like a `ChatMessage`) and **returns just the text**.\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> ‚ÄúGive me just the plain string response from the model ‚Äî no wrappers, no formatting, no role metadata.‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Why Do You Need It?\n",
        "\n",
        "LangChain breaks the LLM flow into modular parts:\n",
        "\n",
        "1. **Prompt** ‚Üí formats the input\n",
        "2. **LLM** ‚Üí generates the response (usually a `ChatMessage`)\n",
        "3. **OutputParser** ‚Üí extracts usable content (e.g., string, JSON, dict)\n",
        "\n",
        "Without a parser, you'd get back something like:\n",
        "\n",
        "```python\n",
        "AIMessage(content=\"Sure, here's a joke...\")\n",
        "```\n",
        "\n",
        "But with `StrOutputParser`, you get:\n",
        "\n",
        "```python\n",
        "\"Sure, here's a joke...\"\n",
        "```\n",
        "\n",
        "This makes the result **clean** and easy to print, log, or send to a UI.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Example in Context\n",
        "\n",
        "Here‚Äôs how it fits into your pipeline:\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "parser = StrOutputParser()\n",
        "\n",
        "# Chain them together using `|`\n",
        "chain = prompt | llm | parser\n",
        "\n",
        "response = chain.invoke({\"topic\": \"AI\"})\n",
        "print(response)\n",
        "```\n",
        "\n",
        "Without `StrOutputParser`, the result would be a `ChatMessage` object. With it, it‚Äôs just a **clean string**.\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Summary\n",
        "\n",
        "| Concept           | Description                                        |\n",
        "| ----------------- | -------------------------------------------------- |\n",
        "| `StrOutputParser` | Extracts the `content` field from an LLM response  |\n",
        "| Input             | `ChatMessage` or model response                    |\n",
        "| Output            | Plain text (`str`)                                 |\n",
        "| Use Case          | Clean up the LLM output for display or further use |\n",
        "\n",
        "---\n",
        "\n",
        "Later on, you‚Äôll use **other output parsers** too:\n",
        "\n",
        "* `PydanticOutputParser` ‚Üí when you want structured JSON objects\n",
        "* `RetryWithErrorOutputParser` ‚Üí for validating outputs\n",
        "* Custom parsers for tools, agents, etc.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xAYaAfkxFwCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, `from langchain_openai import ChatOpenAI` is the LangChain **wrapper for OpenAI's chat models**, like `gpt-3.5-turbo` or `gpt-4`. But LangChain is **model-agnostic**, meaning you can plug in many different LLM providers the same way.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† What `ChatOpenAI` Does\n",
        "\n",
        "This class:\n",
        "\n",
        "* Talks to OpenAI‚Äôs API\n",
        "* Sends structured prompts (`system`, `user`, `assistant`)\n",
        "* Returns the model‚Äôs response\n",
        "\n",
        "LangChain just wraps it so it plays nicely with the rest of the chain (prompts, memory, tools, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üåç Other Model Wrappers in LangChain\n",
        "\n",
        "Yes ‚Äî there are equivalents for most major LLMs. Here‚Äôs how it breaks down:\n",
        "\n",
        "| Model Provider         | LangChain Class                                         | Module to Import                                           |\n",
        "| ---------------------- | ------------------------------------------------------- | ---------------------------------------------------------- |\n",
        "| **OpenAI**             | `ChatOpenAI`                                            | `langchain_openai`                                         |\n",
        "| **Anthropic (Claude)** | `ChatAnthropic`                                         | `langchain_anthropic`                                      |\n",
        "| **Google (Gemini)**    | `ChatVertexAI` or `ChatGoogleGenerativeAI`              | `langchain_google_vertexai` or `langchain_google_genai`    |\n",
        "| **Mistral**            | `ChatMistralAI` or `ChatMistral`                        | `langchain_mistralai` or `langchain_community.chat_models` |\n",
        "| **Llama (Meta)**       | Usually used via `llama.cpp`, Hugging Face, or `ollama` | `langchain_community.chat_models.ChatOllama`               |\n",
        "| **Cohere**             | `ChatCohere`                                            | `langchain_cohere`                                         |\n",
        "| **Together.AI**        | `ChatTogether`                                          | `langchain_community.chat_models`                          |\n",
        "| **Fireworks**          | `ChatFireworks`                                         | `langchain_fireworks`                                      |\n",
        "\n",
        "These all follow the **same interface** as `ChatOpenAI`, so you can swap them out easily with minor config changes.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ Swap OpenAI for Another Model\n",
        "\n",
        "### Example: Using Anthropic‚Äôs Claude\n",
        "\n",
        "```python\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "llm = ChatAnthropic(model=\"claude-3-opus\")\n",
        "```\n",
        "\n",
        "### Example: Using Local Llama via Ollama\n",
        "\n",
        "```python\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "\n",
        "llm = ChatOllama(model=\"llama3\")\n",
        "```\n",
        "\n",
        "### Example: Using Google Gemini Pro\n",
        "\n",
        "```python\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß± Takeaway\n",
        "\n",
        "| Component          | Purpose                                                       |\n",
        "| ------------------ | ------------------------------------------------------------- |\n",
        "| `ChatOpenAI`       | LangChain's interface to OpenAI‚Äôs chat models                 |\n",
        "| Other chat classes | Provide a unified API to other providers                      |\n",
        "| Why it matters     | You can **swap providers** without rewriting your entire app! |\n",
        "\n"
      ],
      "metadata": {
        "id": "feLoBxWiGf8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üß† What Is the Pipe (`|`) Operator in LangChain?\n",
        "\n",
        "In LangChain, the pipe operator lets you **chain together components** like:\n",
        "\n",
        "```python\n",
        "prompt | llm | parser\n",
        "```\n",
        "\n",
        "This creates a **Runnable** ‚Äî a composable, functional pipeline that passes data from one step to the next.\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Why Use a Pipe?\n",
        "\n",
        "It‚Äôs:\n",
        "\n",
        "* ‚úÖ **Readable**: mirrors natural data flow\n",
        "* ‚úÖ **Modular**: lets you swap in/out components easily\n",
        "* ‚úÖ **Scalable**: supports arbitrarily complex chains (with memory, agents, tools, etc.)\n",
        "\n",
        "LangChain‚Äôs design philosophy is:\n",
        "\n",
        "> *Treat every component as a function: `input ‚Üí output`.*\n",
        "\n",
        "The pipe combines these \"functions\" into a **single Runnable chain**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Under the Hood: Functional Programming\n",
        "\n",
        "LangChain components like `ChatPromptTemplate`, `ChatOpenAI`, and `StrOutputParser` all implement a common interface: `Runnable`.\n",
        "\n",
        "So when you do:\n",
        "\n",
        "```python\n",
        "chain = prompt | llm | parser\n",
        "```\n",
        "\n",
        "It‚Äôs equivalent to doing this manually:\n",
        "\n",
        "```python\n",
        "formatted_prompt = prompt.invoke({\"topic\": \"AI\"})\n",
        "response = llm.invoke(formatted_prompt)\n",
        "final_output = parser.invoke(response)\n",
        "```\n",
        "\n",
        "The pipe just **composes** those function calls for you.\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ What Can Be Piped?\n",
        "\n",
        "Any object that implements the `Runnable` interface, including:\n",
        "\n",
        "| Component Type   | Example                        |\n",
        "| ---------------- | ------------------------------ |\n",
        "| Prompt templates | `ChatPromptTemplate`           |\n",
        "| Language models  | `ChatOpenAI`, `ChatAnthropic`  |\n",
        "| Parsers          | `StrOutputParser`, custom ones |\n",
        "| Memory           | `ConversationBufferMemory`     |\n",
        "| Chains           | Custom `RunnableSequence`s     |\n",
        "| Tools/agents     | Toolchains, ReAct agents       |\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ You Can Pipe More Than Three Things\n",
        "\n",
        "It‚Äôs not limited to 3 components. This is valid too:\n",
        "\n",
        "```python\n",
        "template | retriever | summarizer | llm | parser\n",
        "```\n",
        "\n",
        "Each one takes the output from the previous and passes its result to the next.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Example: Custom Pipe with Side Effects\n",
        "\n",
        "Let‚Äôs say you want to log the prompt before calling the model. You could add a custom `Runnable` in between:\n",
        "\n",
        "```python\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "log_step = RunnableLambda(lambda x: (print(\"Prompt:\", x), x)[1])  # Logs and passes input\n",
        "\n",
        "chain = prompt | log_step | llm | parser\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary\n",
        "\n",
        "| Feature                       | Benefit                           |                                       |\n",
        "| ----------------------------- | --------------------------------- | ------------------------------------- |\n",
        "| \\`                            | \\` operator                       | Connects Runnable components together |\n",
        "| Acts like a function pipeline | `x ‚Üí f(x) ‚Üí g(f(x)) ‚Üí h(g(f(x)))` |                                       |\n",
        "| Clean syntax                  | Easy to read and extend           |                                       |\n",
        "| Core to LangChain             | You‚Äôll use it **everywhere**      |                                       |\n",
        "\n"
      ],
      "metadata": {
        "id": "J5CTxHA7RZvN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You‚Äôre ready to explore one of the most exciting parts of LangChain: **agents**. Agents make your LLMs not just talk, but **think**, **decide**, and **act** ‚Äî like calling tools, doing searches, calculations, or fetching external data.\n",
        "\n",
        "Let‚Äôs first look at some **advanced pipe-based pipelines**, and then see how they look when you plug in an **agent**.\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Advanced Pipe Example (No Agent Yet)\n",
        "\n",
        "Let‚Äôs say you want an AI that:\n",
        "\n",
        "1. Takes in a user question\n",
        "2. Searches documents (via retriever)\n",
        "3. Passes relevant info into a prompt\n",
        "4. Gets a model response\n",
        "5. Parses the output\n",
        "\n",
        "```python\n",
        "chain = (\n",
        "    retriever\n",
        "    | (lambda docs: {\"docs\": docs})                     # Convert output to expected input shape\n",
        "    | prompt_template                                    # Uses \"docs\" + \"question\"\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = chain.invoke({\"question\": \"How does photosynthesis work?\"})\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1UuLOs6JRual"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes ‚Äî **exactly!** üéØ That‚Äôs a textbook **RAG** (Retrieval-Augmented Generation) setup ‚Äî just written using LangChain's elegant pipe-style syntax.\n",
        "\n",
        "Let‚Äôs break it down clearly so you understand *why* this is a RAG and how each part maps to the components of the architecture.\n",
        "\n",
        "---\n",
        "\n",
        "## üîÅ What is RAG?\n",
        "\n",
        "**Retrieval-Augmented Generation** (RAG) is a powerful technique where you:\n",
        "\n",
        "> üß† **Retrieve relevant documents** based on a user‚Äôs query, then\n",
        "> ‚úçÔ∏è **Generate an answer** using both the query and those documents.\n",
        "\n",
        "This bridges the gap between what a model knows **(pretraining)** and what it needs to answer **(current or external info)**.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ LangChain RAG Breakdown\n",
        "\n",
        "Here‚Äôs your pipeline:\n",
        "\n",
        "```python\n",
        "chain = (\n",
        "    retriever\n",
        "    | (lambda docs: {\"docs\": docs})                     \n",
        "    | prompt_template                                    \n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "```\n",
        "\n",
        "### üîπ `retriever`\n",
        "\n",
        "* Accepts the user's **query**\n",
        "* Returns a list of **relevant documents**\n",
        "* These docs are usually from a vector store (e.g., Chroma, FAISS)\n",
        "\n",
        "### üîπ `(lambda docs: {\"docs\": docs})`\n",
        "\n",
        "* A quick shim to **rename the output** into the expected input format for the next step (e.g., if your prompt expects `docs` as a key)\n",
        "* Think of this like: `retrieved_docs ‚Üí {\"docs\": retrieved_docs}`\n",
        "\n",
        "### üîπ `prompt_template`\n",
        "\n",
        "* A `ChatPromptTemplate` that expects inputs like `{\"docs\": ..., \"question\": ...}`\n",
        "* This creates a **final structured prompt** like:\n",
        "\n",
        "  ```\n",
        "  You are a helpful assistant. Use the following documents:\n",
        "\n",
        "  {doc content here}\n",
        "\n",
        "  Question: {question}\n",
        "  ```\n",
        "\n",
        "### üîπ `llm`\n",
        "\n",
        "* Your model (e.g., `ChatOpenAI`) that generates the response\n",
        "\n",
        "### üîπ `StrOutputParser()`\n",
        "\n",
        "* Extracts just the **text content** of the model's response\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Input and Output\n",
        "\n",
        "### Input:\n",
        "\n",
        "```python\n",
        "{\"question\": \"How does photosynthesis work?\"}\n",
        "```\n",
        "\n",
        "### Output:\n",
        "\n",
        "```text\n",
        "\"Photosynthesis is the process by which plants use sunlight to...\"\n",
        "```\n",
        "\n",
        "Behind the scenes, the model is generating this answer **using the retrieved documents**, not just its pretrained knowledge.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Why This Is RAG\n",
        "\n",
        "| RAG Element           | In Your Pipeline           |\n",
        "| --------------------- | -------------------------- |\n",
        "| **Retriever**         | `retriever`                |\n",
        "| **Prompt-building**   | `lambda + prompt_template` |\n",
        "| **Generator**         | `llm`                      |\n",
        "| **Result formatting** | `StrOutputParser()`        |\n",
        "\n",
        "‚úîÔ∏è You‚Äôre using **external knowledge** to ground your LLM\n",
        "‚úîÔ∏è You‚Äôre controlling the **prompt input**\n",
        "‚úîÔ∏è You‚Äôre retrieving **on the fly** at runtime\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XITDC8AcSJjy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hlDuTWPM7PEE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}