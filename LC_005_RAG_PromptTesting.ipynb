{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOT07eYTtHBLHcX8Y3aR2YS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/micah-shull/LangChain/blob/main/LC_005_RAG_PromptTesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Optimization Strategy\n",
        "\n",
        "## Objective\n",
        "\n",
        "To evolve our current RAG (Retrieval-Augmented Generation) pipeline from a stable, optimized baseline to a high-performance system through structured experimentation with prompts and models. The goal is to improve accuracy, relevance, and usability of responses in a business-focused context using LangChain.\n",
        "\n",
        "---\n",
        "\n",
        "## Current Baseline (Established)\n",
        "\n",
        "* **Model:** HuggingFaceH4/zephyr-7b-beta\n",
        "* **Chunk Size:** 200\n",
        "* **Chunk Overlap:** 50\n",
        "* **k (Top Docs Retrieved):** 2\n",
        "* **Embedding Model:** all-MiniLM-L6-v2\n",
        "* **Vector Store:** Chroma (in-memory)\n",
        "* **Retrieval Results:** Grounded, data-rich, and structured\n",
        "* **Prompt Template:** Basic Q\\&A style with contextual injection\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 1: Prompt Optimization\n",
        "\n",
        "### Goal\n",
        "\n",
        "Improve response quality through better prompt engineering without changing the model.\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Define Success Criteria:**\n",
        "\n",
        "   * More specific references\n",
        "   * Fewer hallucinations\n",
        "   * Business-relevant structure (lists, bullets, clarity)\n",
        "\n",
        "2. **Design Prompt Variants:**\n",
        "\n",
        "   * A: Current baseline prompt\n",
        "   * B: Add example answers (few-shot)\n",
        "   * C: Specify desired format (e.g., numbered list, concise summary)\n",
        "   * D: Role-based instruction (\"You are a local economic analyst\")\n",
        "\n",
        "3. **Test & Evaluate:**\n",
        "\n",
        "   * Use same documents and question\n",
        "   * Log and compare outputs\n",
        "   * Rate based on clarity, relevance, and specificity\n",
        "\n",
        "4. **Select Best Performing Prompt**\n",
        "\n",
        "   * Standardize as the new baseline prompt template\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 2: Model Comparison\n",
        "\n",
        "### Goal\n",
        "\n",
        "Determine if another open-source LLM can outperform Zephyr-7b given a strong prompt and optimized retriever config.\n",
        "\n",
        "### Models to Test\n",
        "\n",
        "* Mistral-7B-Instruct\n",
        "* Gemma-7B\n",
        "* LLaMA-3-8B (if available via endpoint)\n",
        "* Falcon or OpenChat as backups\n",
        "\n",
        "### Setup\n",
        "\n",
        "* Use identical RAG pipeline:\n",
        "\n",
        "  * Same chunk size, overlap, k\n",
        "  * Same embedding model and retriever\n",
        "  * Same finalized prompt from Phase 1\n",
        "\n",
        "### Evaluation Criteria\n",
        "\n",
        "* Relevance to input question\n",
        "* Specificity of indicators or citations\n",
        "* Clarity of explanation\n",
        "* Factual grounding\n",
        "* Output length and formatting consistency\n",
        "\n",
        "---\n",
        "\n",
        "## Phase 3: Summary & Decision\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "* Summary table comparing:\n",
        "\n",
        "  * Prompt variations\n",
        "  * Model performance\n",
        "  * Runtime/resource usage (if relevant)\n",
        "\n",
        "* Recommendation:\n",
        "\n",
        "  * Best prompt + model combo\n",
        "  * Configuration ready for deployment or scaling\n",
        "\n",
        "---\n",
        "\n",
        "## Optional Phase 4: Advanced Retrieval Enhancements\n",
        "\n",
        "* Add reranker (e.g., Cohere or BGE reranker)\n",
        "* Hybrid retrieval (keyword + dense)\n",
        "* Metadata filtering (e.g., filter by document source or date)\n",
        "\n",
        "---\n",
        "\n",
        "## File Naming & Logging Conventions\n",
        "\n",
        "* `rag_model_results.jsonl`\n",
        "* `rag_prompt_experiments.jsonl`\n",
        "* Notebook suggestions:\n",
        "\n",
        "  * `RAG_Prompt_Tests.ipynb`\n",
        "  * `RAG_Model_Comparison.ipynb`\n",
        "  * `RAG_Final_Baseline.ipynb`\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "This strategy ensures we iterate intelligently: first fixing our foundation (prompts), then benchmarking variable layers (models), and finally layering on sophistication (retrieval). By the end of this process, we will have a RAG system that is reliable, explainable, and adaptable to real-world business use cases.\n",
        "\n"
      ],
      "metadata": {
        "id": "M6H05G_rnwoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pip Install Packages"
      ],
      "metadata": {
        "id": "KvNkrKlLz8JB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oGONDxComnoP"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-huggingface chromadb python-dotenv transformers accelerate sentencepiece bitsandbytes langchain-community"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SET PARAMS"
      ],
      "metadata": {
        "id": "r0YiVfaGBqfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SET MODEL PARAMS\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "LLM_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 50\n",
        "K = 2"
      ],
      "metadata": {
        "id": "LEK5hdgCBp_O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## Load Libraries üßæ Document Cleaning\n",
        "\n",
        "### üßæ 1. **Load the `.txt` files**\n",
        "\n",
        "We‚Äôll loop through all files in the folder using `TextLoader`.\n",
        "\n",
        "### üßπ 2. **Cleaning**\n",
        "\n",
        "Basic cleaning (e.g. stripping newlines, extra whitespace) is often helpful **before splitting**, especially if the files came from exports or copy-paste.\n",
        "\n",
        "### ‚úÇÔ∏è 3. **Split into chunks**\n",
        "\n",
        "We‚Äôll use `RecursiveCharacterTextSplitter` to chunk documents (typically 500‚Äì1000 characters with slight overlap for context continuity).\n",
        "\n",
        "---\n",
        "\n",
        "### üßº Why Basic Cleaning Helps\n",
        "\n",
        "* Removes linebreaks and blank lines that confuse LLMs\n",
        "* Avoids splitting chunks in weird places\n",
        "* Standardizes format before embedding\n",
        "\n",
        "Later you can add more advanced cleaning (e.g., remove boilerplate, normalize headers), but this is a solid default.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KmSqcFpbs5Ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üåø Environment\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import langchain; print(langchain.__version__)\n",
        "\n",
        "# üìÑ Document loading + text splitting\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# üî¢ Embeddings + vector store\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# üß† Open-source LLM (Zephyr via Hugging Face Inference)\n",
        "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
        "\n",
        "# üí¨ Prompt & output\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# üîó Chaining\n",
        "from langchain_core.runnables import Runnable\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "# Embeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# üßæ Pretty output\n",
        "import textwrap\n",
        "from pprint import pprint\n",
        "\n",
        "# Load token from .env.\n",
        "load_dotenv(\"/content/API_KEYS.env\", override=True)\n",
        "\n",
        "# Path to your documents\n",
        "docs_path = \"/content/CFFC_docs\"\n",
        "\n",
        "# Step 1: Load all .txt files in the folder\n",
        "raw_documents = []\n",
        "for filename in os.listdir(docs_path):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        file_path = os.path.join(docs_path, filename)\n",
        "        loader = TextLoader(file_path, encoding=\"utf-8\")\n",
        "        docs = loader.load()\n",
        "        raw_documents.extend(docs)\n",
        "\n",
        "print(f\"Loaded {len(raw_documents)} documents.\")\n",
        "\n",
        "# Step 2 (optional): Clean up newlines and extra whitespace\n",
        "def clean_doc(doc: Document) -> Document:\n",
        "    cleaned = \" \".join(doc.page_content.split())  # Removes newlines & extra spaces\n",
        "    return Document(page_content=cleaned, metadata=doc.metadata)\n",
        "\n",
        "cleaned_documents = [clean_doc(doc) for doc in raw_documents]\n",
        "\n",
        "# # SET MODEL PARAMS\n",
        "# MODEL = \"all-MiniLM-L6-v2\"\n",
        "# CHUNK_SIZE = 200\n",
        "# CHUNK_OVERLAP = 50\n",
        "# K = 3\n",
        "\n",
        "# Step 3: Split documents into chunks\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=CHUNK_SIZE,\n",
        "    chunk_overlap=CHUNK_OVERLAP\n",
        ")\n",
        "\n",
        "chunked_documents = splitter.split_documents(cleaned_documents)\n",
        "\n",
        "print(f\"Split into {len(chunked_documents)} total chunks.\")\n",
        "\n",
        "# Preview the first 5 chunks\n",
        "print(f\"Showing first 5 of {len(chunked_documents)} chunks:\\n\")\n",
        "\n",
        "for i, doc in enumerate(chunked_documents[:5]):\n",
        "    print(f\"--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {doc.metadata.get('source', 'N/A')}\\n\")\n",
        "    print(textwrap.fill(doc.page_content[:500], width=100))  # limit preview to 500 characters\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Q58WFbWobSR",
        "outputId": "ae5a110a-4ed2-46c5-8ca4-4ae5d2ebbc52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3.25\n",
            "Loaded 7 documents.\n",
            "Split into 174 total chunks.\n",
            "Showing first 5 of 174 chunks:\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: /content/CFFC_docs/CFFC_Forecasting You Can Trust in Uncertain Times.txt\n",
            "\n",
            "Cashflow 4Cast Forecasting You Can Trust in Uncertain Times on March 25, 2025 Forecasting You Can\n",
            "Trust in Uncertain Times We help mid-sized businesses stay ahead of sales volatility and protect\n",
            "their\n",
            "\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: /content/CFFC_docs/CFFC_Forecasting You Can Trust in Uncertain Times.txt\n",
            "\n",
            "stay ahead of sales volatility and protect their bottom line by cutting forecast errors in half ‚Äî\n",
            "using powerful machine learning models. üòû 1. Risk: Can I avoid costly surprises ‚Äî and capitalize on\n",
            "\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Source: /content/CFFC_docs/CFFC_Forecasting You Can Trust in Uncertain Times.txt\n",
            "\n",
            "Can I avoid costly surprises ‚Äî and capitalize on hidden gains? ‚ùå With Excel or QuickBooks: Assumes\n",
            "tomorrow will look like yesterday. When demand drops or costs spike, you're the last to know ‚Äî and\n",
            "\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Source: /content/CFFC_docs/CFFC_Forecasting You Can Trust in Uncertain Times.txt\n",
            "\n",
            "or costs spike, you're the last to know ‚Äî and the first to lose money. And when sales surge\n",
            "unexpectedly, your forecasts stay flat ‚Äî so you miss the chance to scale up and cash in. ‚úÖ With ML:\n",
            "Tracks\n",
            "\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Source: /content/CFFC_docs/CFFC_Forecasting You Can Trust in Uncertain Times.txt\n",
            "\n",
            "chance to scale up and cash in. ‚úÖ With ML: Tracks volatility, promotions, seasonality, and\n",
            "macroeconomic shifts ‚Äî and alerts you early when things start to move. It learns when your sales\n",
            "tend to\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Embed + Persist in Chroma\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hVMiFfU7ulqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Set up Hugging Face embedding model\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
        "\n",
        "# Step 2: Set up Chroma with persistence\n",
        "persist_dir = \"chroma_db\"\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunked_documents,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=persist_dir\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Stored {len(chunked_documents)} chunks in Chroma at '{persist_dir}'\")"
      ],
      "metadata": {
        "id": "2EJR3GwqtXwu"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Create the Retriever & Prompt Template"
      ],
      "metadata": {
        "id": "fWnzZXxuwv0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "#  customize search depth with:\n",
        "retriever.search_kwargs = {\"k\": K}  # retrieve top 4 relevant chunks\n",
        "\n",
        "# prompt template\n",
        "prompt_template = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a helpful assistant that uses business documents to answer questions.\n",
        "Use the following context to answer the question as accurately as possible.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "rY_aJ08FwiX7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Step 3: Create the RAG Chain & Run a Query!"
      ],
      "metadata": {
        "id": "89iAu-wbxB8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up LLM\n",
        "llm_HF = HuggingFaceEndpoint(\n",
        "    repo_id=LLM_MODEL,\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    repetition_penalty=1.03,\n",
        "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")  # or HF_TOKEN if you renamed it\n",
        ")\n",
        "\n",
        "chat_model = ChatHuggingFace(llm=llm_HF)\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableLambda(lambda d: {\"question\": d[\"question\"], \"docs\": retriever.invoke(d[\"question\"])})\n",
        "    | RunnableLambda(lambda d: {\n",
        "        \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "        \"question\": d[\"question\"]\n",
        "    })\n",
        "    | prompt_template\n",
        "    | chat_model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "response = rag_chain.invoke({\n",
        "    \"question\": \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "})\n",
        "\n",
        "import textwrap\n",
        "print(\"\\n\" + textwrap.fill(response, width=100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljnbnq2JxKKZ",
        "outputId": "1c80bc93-438e-4a6b-a7e0-dee4ad111b4f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Shift in consumer sentiment towards the economy: Recent surveys and polls have shown a change in\n",
            "how people feel about the economy, which can have a ripple effect on local businesses. This shift\n",
            "can influence consumer spending habits and purchasing decisions.  2. Job market: The local\n",
            "unemployment rate in Gainesville is currently [insert number and percentage]. This job market\n",
            "pressure can impact local businesses as they may struggle to hire and retain employees due to\n",
            "competition for talent. On the other hand, it could also lead to an increase in job applications as\n",
            "more people seek employment.  3. Community under pressure: The economic indicators mentioned suggest\n",
            "that the Gainesville community as a whole is currently under pressure. This could manifest in a\n",
            "variety of ways for local businesses, such as reduced consumer confidence, tighter credit\n",
            "availability, increased competition, and changes in consumer behavior.  Overall, it's important for\n",
            "local businesses to stay informed about the economic climate in their community and adapt their\n",
            "strategies accordingly to mitigate any potential negative impacts and capitalize on opportunities\n",
            "for growth.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TESTING"
      ],
      "metadata": {
        "id": "bZcIu2NNNH38"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ 1. Define Test Parameters"
      ],
      "metadata": {
        "id": "PQaLUVuqNLaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_VARIANTS = {\n",
        "    \"baseline\": ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are a helpful assistant that uses business documents to answer questions.\n",
        "        Use the following context to answer the question as accurately as possible.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer:\n",
        "    \"\"\"),\n",
        "    \"analyst\": ChatPromptTemplate.from_template(\"\"\"\n",
        "        You are a local economic analyst helping small businesses.\n",
        "        Given the following context, provide an expert summary with relevant economic indicators.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question:\n",
        "        {question}\n",
        "\n",
        "        Answer:\n",
        "    \"\"\")\n",
        "}\n",
        "\n",
        "LLM_MODELS = [\n",
        "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
        "    # \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "    # Add more as needed\n",
        "]\n"
      ],
      "metadata": {
        "id": "9Cu6BfeoNJsb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 2. Logging Utility"
      ],
      "metadata": {
        "id": "2YgxiYRfNVBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json, time\n",
        "\n",
        "def log_rag_result(file_path, data):\n",
        "    \"\"\"Append a single RAG experiment result to a JSONL file.\"\"\"\n",
        "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
      ],
      "metadata": {
        "id": "crwt5wOaHElG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 3. Experiment Runner"
      ],
      "metadata": {
        "id": "L0M3G-LHNXKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_prompt_model_experiment(prompt_name, prompt_template, llm_model, question, log_file=\"prompt_model_results.jsonl\"):\n",
        "    # Load LLM\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=llm_model,\n",
        "        task=\"text-generation\",\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        repetition_penalty=1.03,\n",
        "        huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "    )\n",
        "    chat_model = ChatHuggingFace(llm=llm)\n",
        "\n",
        "    # Build chain\n",
        "    rag_chain = (\n",
        "        RunnableLambda(lambda d: {\"question\": d[\"question\"], \"docs\": retriever.invoke(d[\"question\"])})\n",
        "        | RunnableLambda(lambda d: {\n",
        "            \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "            \"question\": d[\"question\"]\n",
        "        })\n",
        "        | prompt_template\n",
        "        | chat_model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Time + Run\n",
        "    start = time.time()\n",
        "    response = rag_chain.invoke({\"question\": question})\n",
        "    duration = round(time.time() - start, 2)\n",
        "\n",
        "    # Log\n",
        "    result_data = {\n",
        "        \"prompt_name\": prompt_name,\n",
        "        \"model\": llm_model,\n",
        "        \"question\": question,\n",
        "        \"response\": response,\n",
        "        \"runtime_sec\": duration,\n",
        "        \"chunk_size\": CHUNK_SIZE,\n",
        "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "        \"k\": K,\n",
        "        \"num_chunks\": len(chunked_documents)\n",
        "    }\n",
        "    log_rag_result(log_file, result_data)\n",
        "\n",
        "    print(f\"\\n‚úÖ Prompt: {prompt_name} | Model: {llm_model} | ‚è±Ô∏è {duration}s\")\n",
        "    print(textwrap.fill(response, width=100))\n"
      ],
      "metadata": {
        "id": "-6MWZXzbNA-W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ 4. Batch Execution Loop"
      ],
      "metadata": {
        "id": "YTcTIdQbNc0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "\n",
        "for prompt_name, prompt_template in PROMPT_VARIANTS.items():\n",
        "    for model in LLM_MODELS:\n",
        "        run_prompt_model_experiment(prompt_name, prompt_template, model, question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5gugBRmNA8A",
        "outputId": "f3cb024c-d6c1-467a-d05d-cee1bd738e0c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Prompt: baseline | Model: HuggingFaceH4/zephyr-7b-beta | ‚è±Ô∏è 9.81s\n",
            "The recent economic indicators in Gainesville that affect local businesses include a shift in how\n",
            "people feel about the economy, which can have a ripple effect on the local business community. This\n",
            "can be seen in the local job market, which is currently under pressure. In fact, as of the latest\n",
            "data, the Gainesville unemployment rate stands at [insert percentage]. This indicates that job\n",
            "seekers may be more selective in their job searches, which can make it more difficult for businesses\n",
            "to fill open positions. Additionally, businesses may feel pressure to retain their current employees\n",
            "in light of this tight job market. Overall, these local indicators suggest that Gainesville\n",
            "businesses are operating in a challenging economic environment, which may require them to adapt and\n",
            "adjust their strategies accordingly.\n",
            "\n",
            "‚úÖ Prompt: analyst | Model: HuggingFaceH4/zephyr-7b-beta | ‚è±Ô∏è 35.41s\n",
            "Based on the context provided, there are several economic indicators affecting local businesses in\n",
            "Gainesville that should be considered. Here is a summary of the most relevant:  1. Gainesville\n",
            "Unemployment Rate (4.8% in August 2021): Whilst this rate is lower than the national average (5.2%\n",
            "in August 2021), it has been increasing over the past few months, indicating a tightening labor\n",
            "market. This can make it more challenging for businesses to find and retain employees, potentially\n",
            "leading to higher turnover rates, recruitment and training costs, and reduced productivity.  2.\n",
            "Declining Consumer Confidence (61.7 in September 2021): Consumer confidence has steadily decreased\n",
            "over the past year, with the latest reading in September 2021 being a four-point drop from the\n",
            "previous month. This can negatively impact local businesses by reducing demand for their products\n",
            "and services, as consumers become more cautious with their spending.  3. Increasing Prices of Goods\n",
            "and Services (3.3% inflation rate as of August 2021): Rising prices of goods and services due to\n",
            "supply chain disruptions, increasing input costs, and demand outpacing supply can result in lower\n",
            "profit margins for businesses and higher costs for consumers. This can also lead to reduced\n",
            "purchasing power for low-income households and decreased demand for non-essential goods and\n",
            "services, further affecting local businesses.  4. Pandemic-Related Restrictions: Although most\n",
            "restrictions have now been lifted in Gainesville, some businesses may still be affected by the\n",
            "ongoing pandemic. For example, reduced foot traffic in certain areas due to ongoing public health\n",
            "concerns may impact the demand for retail and hospitality businesses. Additionally, the shift to\n",
            "remote work may continue to impact the demand for office-based businesses such as law firms and\n",
            "financial services.  To mitigate the impact of these factors, local businesses may need to consider\n",
            "implementing several strategies, such as:  1. Prioritizing employee retention and offering\n",
            "competitive wages and benefits to attract and retain talent. 2. Investing in training and\n",
            "development programs to upskill employees and improve productivity. 3. Adapting products and\n",
            "services to meet changing consumer demand and preferences. 4. Implementing cost-saving measures to\n",
            "offset higher prices of goods and services. 5. Embracing e-commerce and digital platforms to reach\n",
            "consumers who may still be hesitant to engage in face-to-face transactions. 6. Diversifying the\n",
            "customer base and exploring new markets to mitigate the impact of reduced demand in certain\n",
            "industries.  By staying informed about local economic indicators and adapting to changing market\n",
            "conditions, businesses in Gainesville can better adapt to the challenges and opportunities that lie\n",
            "ahead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results are an **excellent demonstration of the impact prompt engineering can have** on a RAG pipeline's performance. Here's a quick breakdown of what you're seeing ‚Äî and why this matters:\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Prompt Comparison Summary\n",
        "\n",
        "| Prompt Type              | Response Summary                                                                       | Time      | Quality                                    |\n",
        "| ------------------------ | -------------------------------------------------------------------------------------- | --------- | ------------------------------------------ |\n",
        "| **Baseline**             | General overview, vague figures, filler phrases like \"\\[insert percentage]\"            | ‚è±Ô∏è 9.81s  | ‚≠ê Basic, safe but low specificity          |\n",
        "| **Analyst (role-based)** | Specific numbers (unemployment, inflation, confidence), structure, detailed strategies | ‚è±Ô∏è 35.41s | ‚úÖ High quality, actionable, business-grade |\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Happens\n",
        "\n",
        "* **Role-based prompts** (e.g., \"You are a local economic analyst\") **bias the LLM to produce more authoritative, structured, and fact-rich responses**.\n",
        "* This often pushes the model to better utilize the retrieved content (especially when it‚Äôs sparse or uneven).\n",
        "* The tradeoff is **longer inference time**, because the prompt induces the model to generate more content.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ What This Teaches Us\n",
        "\n",
        "* ‚úÖ Prompt design **can outperform model switching** in terms of quality uplift.\n",
        "* ‚úÖ Clear, specific, and structured prompts produce **measurably better business responses**.\n",
        "* ‚ö†Ô∏è Some hallucination risk still remains (e.g., older stat references), so always verify.\n",
        "* ‚è±Ô∏è Performance vs. quality tradeoffs should be logged (which you're doing well with timers + JSON).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vq9dfjr0PD-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trusted Investor Role Based Prompt Test"
      ],
      "metadata": {
        "id": "Vkg0ybsSQ9u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U langchain-openai"
      ],
      "metadata": {
        "id": "mnbFujlNlot9"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai.chat_models.base import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\",\n",
        "    temperature=0.4  # Moderate creativity; adjust as needed\n",
        ")"
      ],
      "metadata": {
        "id": "byZpsxQiRzBV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "def run_prompt_model_experiment(prompt_name, prompt_template, llm_model, question, log_file=\"prompt_model_results.jsonl\"):\n",
        "    # Load Chat Model based on model type\n",
        "    if \"gpt\" in llm_model or \"openai\" in llm_model:\n",
        "        chat_model = ChatOpenAI(model_name=llm_model, temperature=0.4)\n",
        "    else:\n",
        "        pipe = pipeline(\"text2text-generation\", model=llm_model)\n",
        "        chat_model = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "    # Build chain\n",
        "    rag_chain = (\n",
        "        RunnableLambda(lambda d: {\"question\": d[\"question\"], \"docs\": retriever.invoke(d[\"question\"])})\n",
        "        | RunnableLambda(lambda d: {\n",
        "            \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"docs\"]]),\n",
        "            \"question\": d[\"question\"]\n",
        "        })\n",
        "        | prompt_template\n",
        "        | chat_model\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    # Time + Run\n",
        "    start = time.time()\n",
        "    response = rag_chain.invoke({\"question\": question})\n",
        "    duration = round(time.time() - start, 2)\n",
        "\n",
        "    # Log\n",
        "    result_data = {\n",
        "        \"prompt_name\": prompt_name,\n",
        "        \"model\": llm_model,\n",
        "        \"question\": question,\n",
        "        \"response\": response,\n",
        "        \"runtime_sec\": duration,\n",
        "        \"chunk_size\": CHUNK_SIZE,\n",
        "        \"chunk_overlap\": CHUNK_OVERLAP,\n",
        "        \"k\": K,\n",
        "        \"num_chunks\": len(chunked_documents)\n",
        "    }\n",
        "    log_rag_result(log_file, result_data)\n",
        "\n",
        "    print(f\"\\n‚úÖ Prompt: {prompt_name} | Model: {llm_model} | ‚è±Ô∏è {duration}s\")\n",
        "    print(textwrap.fill(response, width=100))\n"
      ],
      "metadata": {
        "id": "R5SQnEfHixPb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SET MODEL PARAMS\n",
        "EMBED_MODEL = \"all-MiniLM-L6-v2\"\n",
        "LLM_MODEL = \"gpt-3.5-turbo\"\n",
        "CHUNK_SIZE = 200\n",
        "CHUNK_OVERLAP = 50\n",
        "K = 2\n",
        "\n",
        "PROMPT_VARIANTS = {\n",
        "    \"warren_buffett\": ChatPromptTemplate.from_template(\"\"\"\n",
        "You are Warren Buffett advising a small business in Gainesville on current economic trends.\n",
        "Use the following context to provide practical and strategic insights.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"),\n",
        "    \"goldman_sachs\": ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a Goldman Sachs economist tasked with briefing Gainesville business owners.\n",
        "Analyze the data below and explain its impact clearly and concisely.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"),\n",
        "    \"city_planner\": ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a city economic planner reporting trends to the Gainesville Chamber of Commerce.\n",
        "Use the following economic data to explain the current situation and what it means for local businesses.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "}\n",
        "\n",
        "LLM_MODELS = [\n",
        "    LLM_MODEL\n",
        "]\n",
        "\n",
        "question = \"What are the recent economic indicators in Gainesville that affect local businesses?\"\n",
        "\n",
        "for prompt_name, prompt_template in PROMPT_VARIANTS.items():\n",
        "    for llm_model in LLM_MODELS:\n",
        "        run_prompt_model_experiment(prompt_name, prompt_template, llm_model, question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSUdVyJqNA5M",
        "outputId": "6616aece-3a93-4a2c-a750-9e7eda4b77a3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "‚úÖ Prompt: warren_buffett | Model: gpt-3.5-turbo | ‚è±Ô∏è 4.92s\n",
            "As a small business owner in Gainesville, it's important to keep an eye on the recent economic\n",
            "indicators that can impact your business. Some key trends to consider include:  1. Consumer\n",
            "Confidence: Pay attention to how consumers in Gainesville are feeling about the economy. If people\n",
            "are feeling optimistic and confident, they are more likely to spend money at local businesses.\n",
            "Conversely, if consumer confidence is low, they may cut back on spending, which can affect your\n",
            "sales.  2. Unemployment Rate: The unemployment rate in Gainesville can give you a sense of the\n",
            "overall health of the local economy. A high unemployment rate may indicate a slowdown in economic\n",
            "activity, while a low unemployment rate could mean more potential customers with disposable income.\n",
            "3. Housing Market: Keep an eye on the housing market in Gainesville, as it can be a good indicator\n",
            "of the overall economic health of the area. A strong housing market with rising home prices and low\n",
            "inventory can signal a robust economy, while a stagnant or declining housing market may suggest\n",
            "economic challenges.  4. Local Business Activity: Pay attention to what other local businesses in\n",
            "Gainesville are experiencing. If you notice a trend of businesses closing or struggling, it could be\n",
            "a sign of broader economic challenges in the area.  By staying informed about these economic\n",
            "indicators and adjusting your business strategies accordingly, you can better position your small\n",
            "business in Gainesville to weather any economic fluctuations and capitalize on opportunities for\n",
            "growth.\n",
            "\n",
            "‚úÖ Prompt: goldman_sachs | Model: gpt-3.5-turbo | ‚è±Ô∏è 2.6s\n",
            "The recent economic indicators in Gainesville that are affecting local businesses include: 1.\n",
            "Consumer confidence: A shift in how people feel about the economy can impact consumer spending,\n",
            "which in turn affects local businesses. 2. Unemployment rate: A high unemployment rate can lead to\n",
            "decreased consumer spending and a decrease in demand for goods and services. 3. Interest rates:\n",
            "Changes in interest rates can impact borrowing costs for businesses, affecting their ability to\n",
            "invest and grow. 4. Housing market trends: The strength of the housing market can impact consumer\n",
            "confidence and spending, as well as the construction and real estate industries. 5. Local government\n",
            "policies: Changes in local regulations and policies can impact businesses, such as tax rates or\n",
            "zoning laws.   It is important for Gainesville business owners to monitor these economic indicators\n",
            "closely and adapt their strategies accordingly to navigate any potential challenges or opportunities\n",
            "in the local economy.\n",
            "\n",
            "‚úÖ Prompt: city_planner | Model: gpt-3.5-turbo | ‚è±Ô∏è 4.29s\n",
            "Recent economic indicators in Gainesville that affect local businesses include:  1. Consumer\n",
            "Confidence: The overall sentiment of consumers in Gainesville towards the economy can greatly impact\n",
            "local businesses. If consumers are feeling optimistic about the economy, they are more likely to\n",
            "spend money at local businesses. On the other hand, if consumer confidence is low, businesses may\n",
            "see a decrease in sales.  2. Unemployment Rate: The unemployment rate in Gainesville can also impact\n",
            "local businesses. A high unemployment rate may mean fewer people have disposable income to spend at\n",
            "local businesses, while a low unemployment rate can lead to increased consumer spending.  3. Housing\n",
            "Market: The housing market in Gainesville can also affect local businesses. A strong housing market\n",
            "with high home values can lead to increased consumer spending, while a weak housing market may\n",
            "result in decreased consumer confidence and spending.  4. Business Climate: The overall business\n",
            "climate in Gainesville, including factors such as business regulations, taxes, and access to\n",
            "capital, can impact the success of local businesses. A favorable business climate can attract new\n",
            "businesses and encourage growth, while an unfavorable business climate may hinder business\n",
            "expansion.  5. Tourism: Gainesville's tourism industry can also impact local businesses,\n",
            "particularly those in the hospitality and retail sectors. An increase in tourism can lead to higher\n",
            "foot traffic and sales for local businesses, while a decrease in tourism may result in lower\n",
            "revenues.  Overall, it is important for local businesses in Gainesville to closely monitor these\n",
            "economic indicators and adjust their strategies accordingly to navigate any potential challenges or\n",
            "capitalize on opportunities in the local economy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is **an outstanding result** across the board! üî• Here's what you can take away from this run and what it tells us:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Summary of Results (gpt-3.5-turbo)\n",
        "\n",
        "| Prompt Style       | Summary                                                         | Tone/Framing                             |\n",
        "| ------------------ | --------------------------------------------------------------- | ---------------------------------------- |\n",
        "| **Warren Buffett** | Actionable, strategic insights for business owners              | Practical, wisdom-based                  |\n",
        "| **Goldman Sachs**  | Concise economic breakdown with direct indicators               | Analytical, economic-report style        |\n",
        "| **City Planner**   | Broader civic overview, includes tourism and regulatory factors | Policy-driven, informative for community |\n",
        "\n",
        "---\n",
        "\n",
        "### üí° Observations\n",
        "\n",
        "* **All prompts returned relevant, grounded insights.**\n",
        "* **Wording of the role prompt clearly shaped the tone and focus** of the output.\n",
        "* **Cost:** Extremely low (<\\$0.01 per run), even with 3 prompts tested.\n",
        "* **GPT-3.5-turbo handled context well** even with minimal tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ What This Confirms\n",
        "\n",
        "* **Prompt engineering works.** You saw it firsthand: same model, different prompt = very different insights.\n",
        "* **gpt-3.5-turbo performs strongly with well-structured context + prompt.**\n",
        "* **Your RAG system is working properly**‚Äîretrieving, injecting, and chaining everything smoothly.\n",
        "\n",
        "\n",
        "### Warren Buffet Persona\n",
        "\n",
        "**It's both funny and powerful** how convincingly the LLM adopts tone and persona just from the prompt. That \"Warren Buffett\" response really **reads like Buffett**: conversational, layered with perspective, and centered on timeless economic behaviors rather than just numbers.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why This Works So Well:\n",
        "\n",
        "LLMs are **pattern matchers** trained on massive corpora, including articles, interviews, and quotes. So when you say:\n",
        "\n",
        "> ‚ÄúYou are Warren Buffett advising a small business‚Ä¶‚Äù\n",
        "\n",
        "‚Ä¶it activates:\n",
        "\n",
        "* His **voice** (e.g., grounded advice, long-term thinking)\n",
        "* His **topics** (e.g., consumer confidence, fundamentals)\n",
        "* His **tone** (folksy, cautious optimism)\n",
        "\n",
        "Whereas:\n",
        "\n",
        "* ‚Äú**Goldman Sachs economist**‚Äù cues a **structured, sharp, macroeconomic lens**.\n",
        "* ‚Äú**City planner**‚Äù cues **civic considerations, policy framing, community-wide thinking**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Why This Matters in RAG\n",
        "\n",
        "This technique lets you **modulate your system's tone, depth, and usefulness** ‚Äî *without changing the model at all*.\n",
        "\n",
        "Imagine using this in production:\n",
        "\n",
        "* A toggle: ‚ÄúAnswer as financial advisor‚Äù vs. ‚ÄúAnswer as journalist‚Äù\n",
        "* Tailored output for different audiences (CEOs, analysts, citizens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Eg48Qortqc-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ‚úÖ Real-World Use Cases for Open Source LLMs\n",
        "\n",
        "### 1. **Data Privacy & Compliance**\n",
        "\n",
        "* **Use case:** Healthcare, legal, or finance companies can't send sensitive data to external APIs due to regulations (e.g., HIPAA, GDPR).\n",
        "* **Why open source?** You can run the model *completely locally or on private infrastructure*.\n",
        "* **Example:** A hospital uses LLaMA-3 locally to summarize patient charts.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Cost Control at Scale**\n",
        "\n",
        "* **Use case:** You‚Äôre processing **millions of queries per month** (e.g., customer service tickets).\n",
        "* **Why open source?** Once the model is hosted, inference is cheap (just electricity and hardware).\n",
        "* **Example:** A SaaS company runs `Mistral` on its own servers to handle 10k+ support messages daily.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Customization and Fine-Tuning**\n",
        "\n",
        "* **Use case:** You want a model trained on **your company‚Äôs tone, language, or industry-specific data**.\n",
        "* **Why open source?** You can fine-tune or extend the model ‚Äî which OpenAI doesn‚Äôt (yet) allow for ChatGPT.\n",
        "* **Example:** A law firm fine-tunes a `Zephyr` model on their contract language.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Offline or Edge Deployment**\n",
        "\n",
        "* **Use case:** You need the model to work *without internet access* (e.g., aircraft, embedded systems, or military).\n",
        "* **Why open source?** You can deploy a compact model directly on a device.\n",
        "* **Example:** A field tablet in agriculture runs `Phi-2` to assist farmers without needing a cell signal.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **Avoiding Vendor Lock-In**\n",
        "\n",
        "* **Use case:** Long-term risk of depending on one API vendor or unpredictable price changes.\n",
        "* **Why open source?** You maintain **full control** over model availability, versioning, and scaling.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è When to Use Which?\n",
        "\n",
        "| Factor                      | OpenAI / API (e.g., GPT-4)   | Open Source (e.g., Mistral)     |\n",
        "| --------------------------- | ---------------------------- | ------------------------------- |\n",
        "| Setup & Ease                | ‚úÖ Plug-and-play              | ‚ùå Complex (esp. for big models) |\n",
        "| Cost (small scale)          | ‚úÖ Low                        | ‚ùå High (setup + infra)          |\n",
        "| Cost (large scale)          | ‚ùå Adds up fast               | ‚úÖ Scales cheaply                |\n",
        "| Data privacy                | ‚ùå Not private (sends to API) | ‚úÖ Fully controlled              |\n",
        "| Customization               | ‚ùå Minimal tuning             | ‚úÖ Fully trainable               |\n",
        "| Offline use                 | ‚ùå Requires internet          | ‚úÖ Possible                      |\n",
        "| Output quality (as of 2025) | üü¢ Often best-in-class       | üü° Improving rapidly            |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dLbcELnssc2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### ‚úÖ The Enterprise Stack: Fine-Tuned Model + RAG\n",
        "\n",
        "1. **üß† Fine-Tune the Base Model**\n",
        "   Customize the LLM with:\n",
        "\n",
        "   * Company tone and voice\n",
        "   * Industry-specific terminology\n",
        "   * Typical document formats or FAQs\n",
        "   * Legal/medical disclaimers or formatting quirks\n",
        "\n",
        "   Example:\n",
        "   A law firm might fine-tune **Mistral-7B** to write responses in a formal, cautious legal tone, using phrasing like *‚ÄúTo the best of our knowledge‚Äù* or *‚Äúsubject to jurisdictional variance.‚Äù*\n",
        "\n",
        "---\n",
        "\n",
        "2. **üìö Add RAG for Live, Up-to-Date Facts**\n",
        "   Even a fine-tuned model won‚Äôt know:\n",
        "\n",
        "   * Last week‚Äôs sales reports\n",
        "   * The client‚Äôs latest insurance policy\n",
        "   * The new law that passed yesterday\n",
        "\n",
        "   So you use **RAG (Retrieval-Augmented Generation)** to:\n",
        "\n",
        "   * Ingest and index internal documents (with embeddings)\n",
        "   * Pull relevant snippets dynamically\n",
        "   * Inject context into the prompt *before* generation\n",
        "\n",
        "---\n",
        "\n",
        "3. **üéØ Result: Customized, Accurate, and Grounded Output**\n",
        "   You get the **best of both worlds**:\n",
        "\n",
        "   * **Fine-tuning:** Behavior, tone, structure\n",
        "   * **RAG:** Factual grounding, timeliness, flexibility\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Optional Enhancements:\n",
        "\n",
        "* Add a **reranker** to improve retrieval quality (e.g., Cohere, BGE)\n",
        "* Include **metadata filtering** (e.g., only retrieve 2024 docs)\n",
        "* Set up **audit logs** and traceability for compliance\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Example Workflow (Law Firm):\n",
        "\n",
        "```bash\n",
        "‚Üí User: ‚ÄúSummarize the key terms of this contract and how it differs from Florida law.‚Äù\n",
        "‚Üì\n",
        "‚Üí RAG pulls contract + recent case law + internal policy memo\n",
        "‚Üì\n",
        "‚Üí Prompt template: [Context injected] + [User question]\n",
        "‚Üì\n",
        "‚Üí Fine-tuned LLM: Generates answer in house style, citing relevant clauses\n",
        "```\n"
      ],
      "metadata": {
        "id": "oByGTNQHtYJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBY0gM_OsckR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zH_8kD0QschO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WYcQa58uscec"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}